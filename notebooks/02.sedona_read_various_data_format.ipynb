{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Use sedona to read various data format\n",
    "\n",
    "In this tutorial, we will use sedona to read various geospatial data format such as:\n",
    "- geojson\n",
    "- shape file\n",
    "- csv/tsv\n",
    "- pbf\n",
    "- geoparquet\n",
    "\n",
    "We will also evaluate the performance(e.g. storage space, processing speed) of each format"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "2e0837b4717cb931"
  },
  {
   "cell_type": "code",
   "source": [
    "from sedona.spark import *\n",
    "import geopandas as gpd\n",
    "from pyspark.sql.functions import trim, col\n",
    "from pathlib import Path\n",
    "import json\n",
    "\n",
    "from ipyleaflet import Map, basemaps, basemap_to_tiles, MarkerCluster, Marker, AwesomeIcon\n",
    "from ipywidgets import Layout\n",
    "import numpy as np"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-11-25T10:03:58.626528Z",
     "start_time": "2024-11-25T10:03:57.058964Z"
    }
   },
   "id": "ce7c87816a39c1a6",
   "outputs": [],
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24/04/16 11:39:11 WARN Utils: Your hostname, pengfei-Virtual-Machine resolves to a loopback address: 127.0.1.1; using 10.50.2.80 instead (on interface eth0)\n",
      "24/04/16 11:39:11 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      ":: loading settings :: url = jar:file:/home/pengfei/opt/spark/spark-3.3.0/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ivy Default Cache set to: /home/pengfei/.ivy2/cache\n",
      "The jars for the packages stored in: /home/pengfei/.ivy2/jars\n",
      "com.acervera.osm4scala#osm4scala-spark3-shaded_2.12 added as a dependency\n",
      "org.apache.sedona#sedona-spark-shaded-3.0_2.12 added as a dependency\n",
      "org.datasyslab#geotools-wrapper added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-ac2c8d1b-b492-4ae5-b1a9-ab97a329ebdf;1.0\n",
      "\tconfs: [default]\n",
      "\tfound com.acervera.osm4scala#osm4scala-spark3-shaded_2.12;1.0.11 in central\n",
      "\tfound org.apache.sedona#sedona-spark-shaded-3.0_2.12;1.4.1 in central\n",
      "\tfound org.datasyslab#geotools-wrapper;1.4.0-28.2 in central\n",
      ":: resolution report :: resolve 523ms :: artifacts dl 18ms\n",
      "\t:: modules in use:\n",
      "\tcom.acervera.osm4scala#osm4scala-spark3-shaded_2.12;1.0.11 from central in [default]\n",
      "\torg.apache.sedona#sedona-spark-shaded-3.0_2.12;1.4.1 from central in [default]\n",
      "\torg.datasyslab#geotools-wrapper;1.4.0-28.2 from central in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   3   |   0   |   0   |   0   ||   3   |   0   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-ac2c8d1b-b492-4ae5-b1a9-ab97a329ebdf\n",
      "\tconfs: [default]\n",
      "\t0 artifacts copied, 3 already retrieved (0kB/16ms)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24/04/16 11:39:13 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# build a sedona session (sedona = 1.5.1)\n",
    "config = SedonaContext.builder() \\\n",
    "    .appName(\"Sedona with pyspark\") \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .config(\"spark.driver.memory\", \"6g\") \\\n",
    "    .config('spark.jars.packages',\n",
    "            'com.acervera.osm4scala:osm4scala-spark3-shaded_2.12:1.0.11,' \n",
    "            'org.apache.sedona:sedona-spark-shaded-3.0_2.12:1.4.1,' \n",
    "            'org.datasyslab:geotools-wrapper:1.4.0-28.2'). \\\n",
    "     getOrCreate()\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-16T09:39:30.738863671Z",
     "start_time": "2024-04-16T09:39:06.110193449Z"
    }
   },
   "id": "7b1a2689b9abef60"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-25T10:05:14.859305Z",
     "start_time": "2024-11-25T10:05:01.372546Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# build a sedona session with internet\n",
    "config = SedonaContext.builder(). \\\n",
    "    config('spark.jars.packages',\n",
    "           'org.apache.sedona:sedona-spark-3.5_2.13:1.6.1,'\n",
    "           'org.datasyslab:geotools-wrapper:1.6.1-28.2'). \\\n",
    "    config('spark.jars.repositories', 'https://artifacts.unidata.ucar.edu/repository/unidata-all'). \\\n",
    "getOrCreate()"
   ],
   "id": "a86336a17bf38b98",
   "outputs": [],
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "source": [
    "# create a sedona context\n",
    "sedona = SedonaContext.create(config)\n",
    "sc = sedona.sparkContext\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-11-25T10:05:23.406967Z",
     "start_time": "2024-11-25T10:05:16.194140Z"
    }
   },
   "id": "ec2dc3be27ed3c86",
   "outputs": [],
   "execution_count": 3
  },
  {
   "cell_type": "code",
   "source": [
    "# this sets the encoding of shape files\n",
    "sc.setSystemProperty(\"sedona.global.charset\", \"utf8\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-11-25T10:31:14.987339Z",
     "start_time": "2024-11-25T10:31:14.968955Z"
    }
   },
   "id": "cb60751167bf9567",
   "outputs": [],
   "execution_count": 10
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 1 Read/write shape file"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "da4e63a4a87b99cb"
  },
  {
   "cell_type": "code",
   "source": [
    "win_root_dir = \"C:/Users/PLIU/Documents/ubuntu_share/data_set\"\n",
    "lin_root_dir = \"/home/pengfei/data_set\"\n",
    "\n",
    "fr_commune_file_path = f\"{win_root_dir}/kaggle/geospatial/communes_fr_shape\"\n",
    "\n",
    "# read communes shape file\n",
    "fr_commune_rdd = ShapefileReader.readToGeometryRDD(sc, fr_commune_file_path)\n",
    "fr_commune_df = Adapter.toDf(fr_commune_rdd, sedona)\n",
    "fr_commune_df.printSchema()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-11-25T10:08:11.518064Z",
     "start_time": "2024-11-25T10:08:10.583238Z"
    }
   },
   "id": "e4172408db2d7a3b",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- geometry: geometry (nullable = true)\n",
      " |-- insee: string (nullable = true)\n",
      " |-- nom: string (nullable = true)\n",
      " |-- wikipedia: string (nullable = true)\n",
      " |-- surf_ha: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-25T10:31:18.616330Z",
     "start_time": "2024-11-25T10:31:18.518532Z"
    }
   },
   "cell_type": "code",
   "source": "fr_commune_df.show(5)",
   "id": "fa4a13e912e462e6",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "|            geometry|               insee|                 nom|           wikipedia|             surf_ha|\n",
      "+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "|POLYGON ((9.32016...|2B222            ...|Pie-d'Orezza     ...|fr:Pie-d'Orezza  ...|     573.00000000...|\n",
      "|POLYGON ((9.20010...|2B137            ...|Lano             ...|fr:Lano          ...|     824.00000000...|\n",
      "|POLYGON ((9.27757...|2B051            ...|Cambia           ...|fr:Cambia        ...|     833.00000000...|\n",
      "|POLYGON ((9.25119...|2B106            ...|Érone            ...|fr:Érone         ...|     393.00000000...|\n",
      "|POLYGON ((9.28339...|2B185            ...|Oletta           ...|fr:Oletta        ...|    2674.00000000...|\n",
      "+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "execution_count": 11
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Disk usage\n",
    "\n",
    "The shape file use 301 MB disk space"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "507549fa81351e8"
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8.9M\t/home/pengfei/data_set/kaggle/geospatial/communes_fr_shape/communes-20220101.dbf\r\n",
      "4.0K\t/home/pengfei/data_set/kaggle/geospatial/communes_fr_shape/communes-20220101.prj\r\n",
      "4.0K\t/home/pengfei/data_set/kaggle/geospatial/communes_fr_shape/communes-descriptif.txt\r\n",
      "276K\t/home/pengfei/data_set/kaggle/geospatial/communes_fr_shape/communes-20220101.shx\r\n",
      "4.0K\t/home/pengfei/data_set/kaggle/geospatial/communes_fr_shape/LICENCE.txt\r\n",
      "4.0K\t/home/pengfei/data_set/kaggle/geospatial/communes_fr_shape/communes-20220101.cpg\r\n",
      "292M\t/home/pengfei/data_set/kaggle/geospatial/communes_fr_shape/communes-20220101.shp\r\n",
      "301M\t/home/pengfei/data_set/kaggle/geospatial/communes_fr_shape\r\n"
     ]
    }
   ],
   "source": [
    "! du -ah /home/pengfei/data_set/kaggle/geospatial/communes_fr_shape\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-15T14:27:35.598403879Z",
     "start_time": "2024-04-15T14:27:34.466755621Z"
    }
   },
   "id": "d90a99b5ae12e00e"
  },
  {
   "cell_type": "code",
   "source": [
    "from pyspark.sql import DataFrame\n",
    "\n",
    "\n",
    "def get_nearest_commune(df:DataFrame, latitude:str, longitude:str, max_commune_number:int):\n",
    "    temp_table_name:str = \"temp_tab\"\n",
    "    df.createOrReplaceTempView(temp_table_name)\n",
    "    nearest_commune_df = sedona.sql(f\"\"\"\n",
    "     SELECT z.nom as commune_name, z.insee, ST_DistanceSphere(ST_PointFromText('{longitude},{latitude}', ','), z.geometry) AS distance FROM {temp_table_name} as z ORDER BY distance ASC LIMIT {max_commune_number}\n",
    "     \"\"\")\n",
    "    return nearest_commune_df"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-11-25T10:24:10.024048Z",
     "start_time": "2024-11-25T10:24:09.997552Z"
    }
   },
   "id": "2a527d00100f901d",
   "outputs": [],
   "execution_count": 5
  },
  {
   "cell_type": "code",
   "source": [
    "# the gps coordinates for kremlin-Bicetre is 48.8100° N, 2.3539° E\n",
    "\n",
    "kb_latitude = \"48.8100\"\n",
    "kb_longitude = \"2.3539\""
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-11-25T10:24:16.708044Z",
     "start_time": "2024-11-25T10:24:16.690149Z"
    }
   },
   "id": "473adcb6d72b74d4",
   "outputs": [],
   "execution_count": 6
  },
  {
   "cell_type": "code",
   "source": [
    "\n",
    "kb_nearest_shape_df = get_nearest_commune(fr_commune_df,kb_latitude,kb_longitude,10)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-11-25T10:24:19.792999Z",
     "start_time": "2024-11-25T10:24:19.573520Z"
    }
   },
   "id": "5003cd8c6aa042e9",
   "outputs": [],
   "execution_count": 7
  },
  {
   "cell_type": "code",
   "source": [
    "%%time\n",
    "\n",
    "kb_nearest_shape_df.show()\n",
    "kb_nearest_shape_df.count()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-11-25T10:24:24.657560Z",
     "start_time": "2024-11-25T10:24:21.515387Z"
    }
   },
   "id": "d1ea6fde05bbf929",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+-----+------------------+\n",
      "|      commune_name|insee|          distance|\n",
      "+------------------+-----+------------------+\n",
      "|Le Kremlin-Bicêtre|94043|198.60307108585405|\n",
      "|          Gentilly|94037| 798.3521490770968|\n",
      "|           Arcueil|94003|1543.0937442695515|\n",
      "|         Villejuif|94076| 2007.793912679607|\n",
      "|    Ivry-sur-Seine|94041| 2489.634383841373|\n",
      "|            Cachan|94016| 2590.828517555236|\n",
      "|         Montrouge|92049| 2750.714176859015|\n",
      "|           Bagneux|92007| 3462.091511432535|\n",
      "|   Vitry-sur-Seine|94081|3845.1624363327196|\n",
      "|   L'Haÿ-les-Roses|94038| 3942.190017739479|\n",
      "+------------------+-----+------------------+\n",
      "\n",
      "CPU times: total: 0 ns\n",
      "Wall time: 3.13 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 8
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Read write GeoParquet\n",
    "GeoParquet is an **incubating Open Geospatial Consortium (OGC) standard** that adds interoperable geospatial types `(Point, Line, Polygon)` to Parquet. Currently(16/04/2024), the stable version is 1.0.0\n",
    "You can find the official site of geo-parquet [here](https://geoparquet.org/)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "284575701e4b1f43"
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "clean_fr_commune_df = fr_commune_df.withColumn(\"clean_nom\",trim(col(\"nom\"))).withColumn(\"clean_insee\",trim(col(\"insee\"))).drop(\"nom\").drop(\"insee\").withColumnRenamed(\"clean_nom\",\"nom\").withColumnRenamed(\"clean_insee\",\"insee\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-16T09:44:45.700151638Z",
     "start_time": "2024-04-16T09:44:45.528249357Z"
    }
   },
   "id": "5062dc628135f3e0"
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+--------------------+-----------------+-----+\n",
      "|            geometry|           wikipedia|             surf_ha|              nom|insee|\n",
      "+--------------------+--------------------+--------------------+-----------------+-----+\n",
      "|POLYGON ((9.32016...|fr:Pie-d'Orezza  ...|     573.00000000...|     Pie-d'Orezza|2B222|\n",
      "|POLYGON ((9.20010...|fr:Lano          ...|     824.00000000...|             Lano|2B137|\n",
      "|POLYGON ((9.27757...|fr:Cambia        ...|     833.00000000...|           Cambia|2B051|\n",
      "|POLYGON ((9.25119...|fr:Érone         ...|     393.00000000...|            Érone|2B106|\n",
      "|POLYGON ((9.28339...|fr:Oletta        ...|    2674.00000000...|           Oletta|2B185|\n",
      "|POLYGON ((9.30951...|fr:Canari (Haute-...|    1678.00000000...|           Canari|2B058|\n",
      "|POLYGON ((9.30101...|fr:Olmeta-di-Tuda...|    1753.00000000...|   Olmeta-di-Tuda|2B188|\n",
      "|POLYGON ((9.32662...|fr:Campana       ...|     236.00000000...|          Campana|2B052|\n",
      "|POLYGON ((9.33944...|fr:Carcheto-Brust...|     525.00000000...|Carcheto-Brustico|2B063|\n",
      "|POLYGON ((9.34478...|fr:Ampriani      ...|     230.00000000...|         Ampriani|2B015|\n",
      "|POLYGON ((9.33451...|fr:Pianello      ...|    1677.00000000...|         Pianello|2B213|\n",
      "|POLYGON ((9.32604...|fr:Zuani         ...|     518.00000000...|            Zuani|2B364|\n",
      "|POLYGON ((9.33795...|fr:Pietraserena  ...|     678.00000000...|     Pietraserena|2B226|\n",
      "|POLYGON ((9.33181...|fr:Piedipartino  ...|     326.00000000...|     Piedipartino|2B221|\n",
      "|POLYGON ((2.58733...|fr:Montbolo      ...|    2230.00000000...|         Montbolo|66113|\n",
      "|POLYGON ((1.96641...|fr:Targasonne    ...|     787.00000000...|       Targasonne|66202|\n",
      "|POLYGON ((2.86121...|fr:L'Albère      ...|    1620.00000000...|         L'Albère|66001|\n",
      "|POLYGON ((2.11418...|fr:Mont-Louis (Py...|      38.00000000...|       Mont-Louis|66117|\n",
      "|POLYGON ((1.98630...|fr:Estavar       ...|     926.00000000...|          Estavar|66072|\n",
      "|POLYGON ((2.00509...|fr:Égat          ...|     451.00000000...|             Égat|66064|\n",
      "+--------------------+--------------------+--------------------+-----------------+-----+\n"
     ]
    }
   ],
   "source": [
    "clean_fr_commune_df.show()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-16T09:45:00.553487428Z",
     "start_time": "2024-04-16T09:44:58.007347319Z"
    }
   },
   "id": "eba878d7a6f2d1c0"
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "fr_commune_geoparquet_file_path = \"/home/pengfei/data_set/kaggle/geospatial/communes_fr_geoparquet\"\n",
    "clean_fr_commune_df.write.format(\"geoparquet\").option(\"geoparquet.version\",\"1.0.0\").save(fr_commune_geoparquet_file_path)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-16T09:48:00.342320453Z",
     "start_time": "2024-04-16T09:47:49.119326543Z"
    }
   },
   "id": "642ed0cfc2e10ad1"
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\t/home/pengfei/data_set/kaggle/geospatial/communes_fr_geoparquet/_SUCCESS\r\n",
      "2.3M\t/home/pengfei/data_set/kaggle/geospatial/communes_fr_geoparquet/.part-00000-82765f1e-fe4e-4e74-81e2-01fd73bcdb34-c000.snappy.parquet.crc\r\n",
      "4.0K\t/home/pengfei/data_set/kaggle/geospatial/communes_fr_geoparquet/._SUCCESS.crc\r\n",
      "291M\t/home/pengfei/data_set/kaggle/geospatial/communes_fr_geoparquet/part-00000-82765f1e-fe4e-4e74-81e2-01fd73bcdb34-c000.snappy.parquet\r\n",
      "294M\t/home/pengfei/data_set/kaggle/geospatial/communes_fr_geoparquet\r\n"
     ]
    }
   ],
   "source": [
    "! du -ah /home/pengfei/data_set/kaggle/geospatial/communes_fr_geoparquet"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-16T10:24:08.581952184Z",
     "start_time": "2024-04-16T10:24:07.501643375Z"
    }
   },
   "id": "3d937cc14fff10b4"
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [
    "geo_parquet_df = sedona.read.format(\"geoparquet\").load(fr_commune_geoparquet_file_path)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-16T11:15:25.126938309Z",
     "start_time": "2024-04-16T11:15:24.545488491Z"
    }
   },
   "id": "15b7d4ec1ea6b867"
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+--------------------+-----------------+-----+\n",
      "|            geometry|           wikipedia|             surf_ha|              nom|insee|\n",
      "+--------------------+--------------------+--------------------+-----------------+-----+\n",
      "|POLYGON ((9.32016...|fr:Pie-d'Orezza  ...|     573.00000000...|     Pie-d'Orezza|2B222|\n",
      "|POLYGON ((9.20010...|fr:Lano          ...|     824.00000000...|             Lano|2B137|\n",
      "|POLYGON ((9.27757...|fr:Cambia        ...|     833.00000000...|           Cambia|2B051|\n",
      "|POLYGON ((9.25119...|fr:Érone         ...|     393.00000000...|            Érone|2B106|\n",
      "|POLYGON ((9.28339...|fr:Oletta        ...|    2674.00000000...|           Oletta|2B185|\n",
      "|POLYGON ((9.30951...|fr:Canari (Haute-...|    1678.00000000...|           Canari|2B058|\n",
      "|POLYGON ((9.30101...|fr:Olmeta-di-Tuda...|    1753.00000000...|   Olmeta-di-Tuda|2B188|\n",
      "|POLYGON ((9.32662...|fr:Campana       ...|     236.00000000...|          Campana|2B052|\n",
      "|POLYGON ((9.33944...|fr:Carcheto-Brust...|     525.00000000...|Carcheto-Brustico|2B063|\n",
      "|POLYGON ((9.34478...|fr:Ampriani      ...|     230.00000000...|         Ampriani|2B015|\n",
      "|POLYGON ((9.33451...|fr:Pianello      ...|    1677.00000000...|         Pianello|2B213|\n",
      "|POLYGON ((9.32604...|fr:Zuani         ...|     518.00000000...|            Zuani|2B364|\n",
      "|POLYGON ((9.33795...|fr:Pietraserena  ...|     678.00000000...|     Pietraserena|2B226|\n",
      "|POLYGON ((9.33181...|fr:Piedipartino  ...|     326.00000000...|     Piedipartino|2B221|\n",
      "|POLYGON ((2.58733...|fr:Montbolo      ...|    2230.00000000...|         Montbolo|66113|\n",
      "|POLYGON ((1.96641...|fr:Targasonne    ...|     787.00000000...|       Targasonne|66202|\n",
      "|POLYGON ((2.86121...|fr:L'Albère      ...|    1620.00000000...|         L'Albère|66001|\n",
      "|POLYGON ((2.11418...|fr:Mont-Louis (Py...|      38.00000000...|       Mont-Louis|66117|\n",
      "|POLYGON ((1.98630...|fr:Estavar       ...|     926.00000000...|          Estavar|66072|\n",
      "|POLYGON ((2.00509...|fr:Égat          ...|     451.00000000...|             Égat|66064|\n",
      "+--------------------+--------------------+--------------------+-----------------+-----+\n",
      "24/04/16 13:15:50 WARN GeoParquetFileFormat: GeoParquet currently does not support vectorized reader. Falling back to parquet-mr\n",
      "24/04/16 13:15:50 WARN GeoParquetFileFormat: GeoParquet currently does not support vectorized reader. Falling back to parquet-mr\n",
      "24/04/16 13:15:50 WARN GeoParquetFileFormat: GeoParquet currently does not support vectorized reader. Falling back to parquet-mr\n",
      "24/04/16 13:15:50 WARN GeoParquetFileFormat: GeoParquet currently does not support vectorized reader. Falling back to parquet-mr\n"
     ]
    },
    {
     "data": {
      "text/plain": "34955"
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "geo_parquet_df.show()\n",
    "geo_parquet_df.count()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-16T11:15:51.114646330Z",
     "start_time": "2024-04-16T11:15:47.005268357Z"
    }
   },
   "id": "d49a4ba1ca75ee15"
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [],
   "source": [
    "kb_nearest_parquet_df = get_nearest_commune(geo_parquet_df,kb_latitude,kb_longitude,10)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-16T11:26:07.496145072Z",
     "start_time": "2024-04-16T11:26:07.426289738Z"
    }
   },
   "id": "2967242bc44c12a0"
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+-----+------------------+\n",
      "|      commune_name|insee|          distance|\n",
      "+------------------+-----+------------------+\n",
      "|Le Kremlin-Bicêtre|94043|255.77950075329835|\n",
      "|          Gentilly|94037| 1138.204118880015|\n",
      "|         Villejuif|94076|2067.5242470555963|\n",
      "|           Arcueil|94003| 2269.505672821453|\n",
      "|            Cachan|94016|3169.7694895288837|\n",
      "|    Ivry-sur-Seine|94041| 3769.348960915047|\n",
      "|         Montrouge|92049| 4124.301376321017|\n",
      "|   L'Haÿ-les-Roses|94038| 4166.688028197553|\n",
      "|    Chevilly-Larue|94021| 4789.020724647998|\n",
      "|           Bagneux|92007|  5041.99634269013|\n",
      "+------------------+-----+------------------+\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 18:=============================>                            (2 + 2) / 4]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 9.98 ms, sys: 6.56 ms, total: 16.5 ms\n",
      "Wall time: 6.6 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": "10"
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "kb_nearest_parquet_df.show()\n",
    "kb_nearest_parquet_df.count()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-16T11:26:18.436106849Z",
     "start_time": "2024-04-16T11:26:11.802682371Z"
    }
   },
   "id": "e8f5da85bd8a205e"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Custom metadata in geo parquet"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "34ff512153c4ee98"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Compare the result of shape file and geo parquet, we don't gain too many things\n",
    "\n",
    "| file format | disk space | distance (in sec) |\n",
    "|-------------|------------|-------------------|\n",
    "| shape file  | 301        | 7,45              |\n",
    "| geoparquet  | 294        | 6,60              |"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "4902b5fef6331016"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Read write GeoJSON(Geographic JavaScript Object Notation)\n",
    "\n",
    "Sedona can read geojson easily, but can't write geojson. Geo pandas can write geojson. But it can't support large \n",
    "data frame. Below are two examples. In the first, we create a simple geo dataframe. It works without problem.\n",
    "The second does work at all. We have an oom error."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "717a0ad83de39384"
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   id name                 geometry\n",
      "0   1    A  POINT (1.00000 1.00000)\n",
      "1   2    B  POINT (2.00000 2.00000)\n",
      "2   3    C  POINT (3.00000 3.00000)\n"
     ]
    }
   ],
   "source": [
    "from shapely import Point\n",
    "\n",
    "data = {\n",
    "    'id': [1, 2, 3],\n",
    "    'name': ['A', 'B', 'C'],\n",
    "    'geometry': [Point(1, 1), Point(2, 2), Point(3, 3)]\n",
    "}\n",
    "\n",
    "fr_commune_geoj_file_path = \"/home/pengfei/data_set/kaggle/geospatial/communes_fr_geojson.json\"\n",
    "\n",
    "gdf = gpd.GeoDataFrame(data, crs=\"EPSG:4326\")\n",
    "\n",
    "print(gdf.head())\n",
    "\n",
    "# Write GeoDataFrame to GeoJSON file\n",
    "gdf.to_file(fr_commune_geoj_file_path, driver='GeoJSON')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-15T14:29:14.499343553Z",
     "start_time": "2024-04-15T14:29:14.432043733Z"
    }
   },
   "id": "2b4c79db7ae1c04c"
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "|            geometry|               insee|                 nom|           wikipedia|             surf_ha|\n",
      "+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "|POLYGON ((9.32016...|2B222            ...|Pie-d'Orezza     ...|fr:Pie-d'Orezza  ...|     573.00000000...|\n",
      "|POLYGON ((9.20010...|2B137            ...|Lano             ...|fr:Lano          ...|     824.00000000...|\n",
      "|POLYGON ((9.27757...|2B051            ...|Cambia           ...|fr:Cambia        ...|     833.00000000...|\n",
      "|POLYGON ((9.25119...|2B106            ...|Érone            ...|fr:Érone         ...|     393.00000000...|\n",
      "|POLYGON ((9.28339...|2B185            ...|Oletta           ...|fr:Oletta        ...|    2674.00000000...|\n",
      "|POLYGON ((9.30951...|2B058            ...|Canari           ...|fr:Canari (Haute-...|    1678.00000000...|\n",
      "|POLYGON ((9.30101...|2B188            ...|Olmeta-di-Tuda   ...|fr:Olmeta-di-Tuda...|    1753.00000000...|\n",
      "|POLYGON ((9.32662...|2B052            ...|Campana          ...|fr:Campana       ...|     236.00000000...|\n",
      "|POLYGON ((9.33944...|2B063            ...|Carcheto-Brustico...|fr:Carcheto-Brust...|     525.00000000...|\n",
      "|POLYGON ((9.34478...|2B015            ...|Ampriani         ...|fr:Ampriani      ...|     230.00000000...|\n",
      "|POLYGON ((9.33451...|2B213            ...|Pianello         ...|fr:Pianello      ...|    1677.00000000...|\n",
      "|POLYGON ((9.32604...|2B364            ...|Zuani            ...|fr:Zuani         ...|     518.00000000...|\n",
      "|POLYGON ((9.33795...|2B226            ...|Pietraserena     ...|fr:Pietraserena  ...|     678.00000000...|\n",
      "|POLYGON ((9.33181...|2B221            ...|Piedipartino     ...|fr:Piedipartino  ...|     326.00000000...|\n",
      "|POLYGON ((2.58733...|66113            ...|Montbolo         ...|fr:Montbolo      ...|    2230.00000000...|\n",
      "|POLYGON ((1.96641...|66202            ...|Targasonne       ...|fr:Targasonne    ...|     787.00000000...|\n",
      "|POLYGON ((2.86121...|66001            ...|L'Albère         ...|fr:L'Albère      ...|    1620.00000000...|\n",
      "|POLYGON ((2.11418...|66117            ...|Mont-Louis       ...|fr:Mont-Louis (Py...|      38.00000000...|\n",
      "|POLYGON ((1.98630...|66072            ...|Estavar          ...|fr:Estavar       ...|     926.00000000...|\n",
      "|POLYGON ((2.00509...|66064            ...|Égat             ...|fr:Égat          ...|     451.00000000...|\n",
      "+--------------------+--------------------+--------------------+--------------------+--------------------+\n"
     ]
    }
   ],
   "source": [
    "fr_commune_df.show()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-15T14:28:42.200495626Z",
     "start_time": "2024-04-15T14:28:41.861630313Z"
    }
   },
   "id": "14f2e6c68909fd64"
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "outputs": [],
   "source": [
    "from shapely import Polygon\n",
    "from pyspark.sql.functions import collect_list\n",
    "\n",
    "\n",
    "def get_geopandas_df(spark_df:DataFrame):\n",
    "    # Convert Spark DataFrame to Pandas DataFrame\n",
    "    pandas_df = spark_df.toPandas()\n",
    "\n",
    "    # Create a GeoPandas DataFrame from the Pandas DataFrame\n",
    "    # Make sure to create Shapely geometry objects from the geometry column\n",
    "    pandas_df['geometry'] = pandas_df['geometry'].apply(lambda x: Polygon(eval(x)))\n",
    "    geo_df = gpd.GeoDataFrame(pandas_df, geometry='geometry')\n",
    "    \n",
    "    return geo_df"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-15T14:03:02.670598870Z",
     "start_time": "2024-04-15T14:03:02.628166906Z"
    }
   },
   "id": "d2110268a0a6ce4e"
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 21:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24/04/15 16:03:16 ERROR Executor: Exception in task 0.0 in stage 21.0 (TID 19)\n",
      "org.apache.spark.SparkException: Kryo serialization failed: Buffer overflow. Available: 0, required: 67108864. To avoid this, increase spark.kryoserializer.buffer.max value.\n",
      "\tat org.apache.spark.serializer.KryoSerializerInstance.serialize(KryoSerializer.scala:391)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:593)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "Caused by: com.esotericsoftware.kryo.KryoException: Buffer overflow. Available: 0, required: 67108864\n",
      "\tat com.esotericsoftware.kryo.io.Output.require(Output.java:167)\n",
      "\tat com.esotericsoftware.kryo.io.Output.writeBytes(Output.java:251)\n",
      "\tat com.esotericsoftware.kryo.io.Output.writeBytes(Output.java:237)\n",
      "\tat com.esotericsoftware.kryo.serializers.DefaultArraySerializers$ByteArraySerializer.write(DefaultArraySerializers.java:49)\n",
      "\tat com.esotericsoftware.kryo.serializers.DefaultArraySerializers$ByteArraySerializer.write(DefaultArraySerializers.java:38)\n",
      "\tat com.esotericsoftware.kryo.Kryo.writeClassAndObject(Kryo.java:651)\n",
      "\tat com.twitter.chill.Tuple2Serializer.write(TupleSerializers.scala:37)\n",
      "\tat com.twitter.chill.Tuple2Serializer.write(TupleSerializers.scala:33)\n",
      "\tat com.esotericsoftware.kryo.Kryo.writeClassAndObject(Kryo.java:651)\n",
      "\tat com.esotericsoftware.kryo.serializers.DefaultArraySerializers$ObjectArraySerializer.write(DefaultArraySerializers.java:361)\n",
      "\tat com.esotericsoftware.kryo.serializers.DefaultArraySerializers$ObjectArraySerializer.write(DefaultArraySerializers.java:302)\n",
      "\tat com.esotericsoftware.kryo.Kryo.writeClassAndObject(Kryo.java:651)\n",
      "\tat org.apache.spark.serializer.KryoSerializerInstance.serialize(KryoSerializer.scala:387)\n",
      "\t... 4 more\n",
      "24/04/15 16:03:16 WARN TaskSetManager: Lost task 0.0 in stage 21.0 (TID 19) (10.50.2.80 executor driver): org.apache.spark.SparkException: Kryo serialization failed: Buffer overflow. Available: 0, required: 67108864. To avoid this, increase spark.kryoserializer.buffer.max value.\n",
      "\tat org.apache.spark.serializer.KryoSerializerInstance.serialize(KryoSerializer.scala:391)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:593)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "Caused by: com.esotericsoftware.kryo.KryoException: Buffer overflow. Available: 0, required: 67108864\n",
      "\tat com.esotericsoftware.kryo.io.Output.require(Output.java:167)\n",
      "\tat com.esotericsoftware.kryo.io.Output.writeBytes(Output.java:251)\n",
      "\tat com.esotericsoftware.kryo.io.Output.writeBytes(Output.java:237)\n",
      "\tat com.esotericsoftware.kryo.serializers.DefaultArraySerializers$ByteArraySerializer.write(DefaultArraySerializers.java:49)\n",
      "\tat com.esotericsoftware.kryo.serializers.DefaultArraySerializers$ByteArraySerializer.write(DefaultArraySerializers.java:38)\n",
      "\tat com.esotericsoftware.kryo.Kryo.writeClassAndObject(Kryo.java:651)\n",
      "\tat com.twitter.chill.Tuple2Serializer.write(TupleSerializers.scala:37)\n",
      "\tat com.twitter.chill.Tuple2Serializer.write(TupleSerializers.scala:33)\n",
      "\tat com.esotericsoftware.kryo.Kryo.writeClassAndObject(Kryo.java:651)\n",
      "\tat com.esotericsoftware.kryo.serializers.DefaultArraySerializers$ObjectArraySerializer.write(DefaultArraySerializers.java:361)\n",
      "\tat com.esotericsoftware.kryo.serializers.DefaultArraySerializers$ObjectArraySerializer.write(DefaultArraySerializers.java:302)\n",
      "\tat com.esotericsoftware.kryo.Kryo.writeClassAndObject(Kryo.java:651)\n",
      "\tat org.apache.spark.serializer.KryoSerializerInstance.serialize(KryoSerializer.scala:387)\n",
      "\t... 4 more\n",
      "\n",
      "24/04/15 16:03:16 ERROR TaskSetManager: Task 0 in stage 21.0 failed 1 times; aborting job\n"
     ]
    },
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o49.collectToPython.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 21.0 failed 1 times, most recent failure: Lost task 0.0 in stage 21.0 (TID 19) (10.50.2.80 executor driver): org.apache.spark.SparkException: Kryo serialization failed: Buffer overflow. Available: 0, required: 67108864. To avoid this, increase spark.kryoserializer.buffer.max value.\n\tat org.apache.spark.serializer.KryoSerializerInstance.serialize(KryoSerializer.scala:391)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:593)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n\tat java.base/java.lang.Thread.run(Thread.java:829)\nCaused by: com.esotericsoftware.kryo.KryoException: Buffer overflow. Available: 0, required: 67108864\n\tat com.esotericsoftware.kryo.io.Output.require(Output.java:167)\n\tat com.esotericsoftware.kryo.io.Output.writeBytes(Output.java:251)\n\tat com.esotericsoftware.kryo.io.Output.writeBytes(Output.java:237)\n\tat com.esotericsoftware.kryo.serializers.DefaultArraySerializers$ByteArraySerializer.write(DefaultArraySerializers.java:49)\n\tat com.esotericsoftware.kryo.serializers.DefaultArraySerializers$ByteArraySerializer.write(DefaultArraySerializers.java:38)\n\tat com.esotericsoftware.kryo.Kryo.writeClassAndObject(Kryo.java:651)\n\tat com.twitter.chill.Tuple2Serializer.write(TupleSerializers.scala:37)\n\tat com.twitter.chill.Tuple2Serializer.write(TupleSerializers.scala:33)\n\tat com.esotericsoftware.kryo.Kryo.writeClassAndObject(Kryo.java:651)\n\tat com.esotericsoftware.kryo.serializers.DefaultArraySerializers$ObjectArraySerializer.write(DefaultArraySerializers.java:361)\n\tat com.esotericsoftware.kryo.serializers.DefaultArraySerializers$ObjectArraySerializer.write(DefaultArraySerializers.java:302)\n\tat com.esotericsoftware.kryo.Kryo.writeClassAndObject(Kryo.java:651)\n\tat org.apache.spark.serializer.KryoSerializerInstance.serialize(KryoSerializer.scala:387)\n\t... 4 more\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2672)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2608)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2607)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2607)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1182)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1182)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1182)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2860)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2802)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2791)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:952)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2228)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2249)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2268)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2293)\n\tat org.apache.spark.rdd.RDD.$anonfun$collect$1(RDD.scala:1021)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:406)\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:1020)\n\tat org.apache.spark.sql.execution.SparkPlan.executeCollect(SparkPlan.scala:424)\n\tat org.apache.spark.sql.Dataset.$anonfun$collectToPython$1(Dataset.scala:3688)\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$2(Dataset.scala:3858)\n\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:510)\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:3856)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:109)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:169)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:95)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:779)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:3856)\n\tat org.apache.spark.sql.Dataset.collectToPython(Dataset.scala:3685)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:829)\nCaused by: org.apache.spark.SparkException: Kryo serialization failed: Buffer overflow. Available: 0, required: 67108864. To avoid this, increase spark.kryoserializer.buffer.max value.\n\tat org.apache.spark.serializer.KryoSerializerInstance.serialize(KryoSerializer.scala:391)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:593)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n\t... 1 more\nCaused by: com.esotericsoftware.kryo.KryoException: Buffer overflow. Available: 0, required: 67108864\n\tat com.esotericsoftware.kryo.io.Output.require(Output.java:167)\n\tat com.esotericsoftware.kryo.io.Output.writeBytes(Output.java:251)\n\tat com.esotericsoftware.kryo.io.Output.writeBytes(Output.java:237)\n\tat com.esotericsoftware.kryo.serializers.DefaultArraySerializers$ByteArraySerializer.write(DefaultArraySerializers.java:49)\n\tat com.esotericsoftware.kryo.serializers.DefaultArraySerializers$ByteArraySerializer.write(DefaultArraySerializers.java:38)\n\tat com.esotericsoftware.kryo.Kryo.writeClassAndObject(Kryo.java:651)\n\tat com.twitter.chill.Tuple2Serializer.write(TupleSerializers.scala:37)\n\tat com.twitter.chill.Tuple2Serializer.write(TupleSerializers.scala:33)\n\tat com.esotericsoftware.kryo.Kryo.writeClassAndObject(Kryo.java:651)\n\tat com.esotericsoftware.kryo.serializers.DefaultArraySerializers$ObjectArraySerializer.write(DefaultArraySerializers.java:361)\n\tat com.esotericsoftware.kryo.serializers.DefaultArraySerializers$ObjectArraySerializer.write(DefaultArraySerializers.java:302)\n\tat com.esotericsoftware.kryo.Kryo.writeClassAndObject(Kryo.java:651)\n\tat org.apache.spark.serializer.KryoSerializerInstance.serialize(KryoSerializer.scala:387)\n\t... 4 more\n",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mPy4JJavaError\u001B[0m                             Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[24], line 1\u001B[0m\n\u001B[0;32m----> 1\u001B[0m gdf \u001B[38;5;241m=\u001B[39m \u001B[43mget_geopandas_df\u001B[49m\u001B[43m(\u001B[49m\u001B[43mfr_commune_df\u001B[49m\u001B[43m)\u001B[49m\n",
      "Cell \u001B[0;32mIn[23], line 7\u001B[0m, in \u001B[0;36mget_geopandas_df\u001B[0;34m(spark_df)\u001B[0m\n\u001B[1;32m      5\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mget_geopandas_df\u001B[39m(spark_df:DataFrame):\n\u001B[1;32m      6\u001B[0m     \u001B[38;5;66;03m# Convert Spark DataFrame to Pandas DataFrame\u001B[39;00m\n\u001B[0;32m----> 7\u001B[0m     pandas_df \u001B[38;5;241m=\u001B[39m \u001B[43mspark_df\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtoPandas\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m      9\u001B[0m     \u001B[38;5;66;03m# Create a GeoPandas DataFrame from the Pandas DataFrame\u001B[39;00m\n\u001B[1;32m     10\u001B[0m     \u001B[38;5;66;03m# Make sure to create Shapely geometry objects from the geometry column\u001B[39;00m\n\u001B[1;32m     11\u001B[0m     pandas_df[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mgeometry\u001B[39m\u001B[38;5;124m'\u001B[39m] \u001B[38;5;241m=\u001B[39m pandas_df[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mgeometry\u001B[39m\u001B[38;5;124m'\u001B[39m]\u001B[38;5;241m.\u001B[39mapply(\u001B[38;5;28;01mlambda\u001B[39;00m x: Polygon(\u001B[38;5;28meval\u001B[39m(x)))\n",
      "File \u001B[0;32m~/.cache/pypoetry/virtualenvs/geoparquetandsedona-uqJLJxB7-py3.10/lib/python3.10/site-packages/pyspark/sql/pandas/conversion.py:205\u001B[0m, in \u001B[0;36mPandasConversionMixin.toPandas\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    202\u001B[0m             \u001B[38;5;28;01mraise\u001B[39;00m\n\u001B[1;32m    204\u001B[0m \u001B[38;5;66;03m# Below is toPandas without Arrow optimization.\u001B[39;00m\n\u001B[0;32m--> 205\u001B[0m pdf \u001B[38;5;241m=\u001B[39m pd\u001B[38;5;241m.\u001B[39mDataFrame\u001B[38;5;241m.\u001B[39mfrom_records(\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcollect\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m, columns\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcolumns)\n\u001B[1;32m    206\u001B[0m column_counter \u001B[38;5;241m=\u001B[39m Counter(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcolumns)\n\u001B[1;32m    208\u001B[0m corrected_dtypes: List[Optional[Type]] \u001B[38;5;241m=\u001B[39m [\u001B[38;5;28;01mNone\u001B[39;00m] \u001B[38;5;241m*\u001B[39m \u001B[38;5;28mlen\u001B[39m(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mschema)\n",
      "File \u001B[0;32m~/.cache/pypoetry/virtualenvs/geoparquetandsedona-uqJLJxB7-py3.10/lib/python3.10/site-packages/pyspark/sql/dataframe.py:817\u001B[0m, in \u001B[0;36mDataFrame.collect\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    807\u001B[0m \u001B[38;5;250m\u001B[39m\u001B[38;5;124;03m\"\"\"Returns all the records as a list of :class:`Row`.\u001B[39;00m\n\u001B[1;32m    808\u001B[0m \n\u001B[1;32m    809\u001B[0m \u001B[38;5;124;03m.. versionadded:: 1.3.0\u001B[39;00m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    814\u001B[0m \u001B[38;5;124;03m[Row(age=2, name='Alice'), Row(age=5, name='Bob')]\u001B[39;00m\n\u001B[1;32m    815\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m    816\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m SCCallSiteSync(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_sc):\n\u001B[0;32m--> 817\u001B[0m     sock_info \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_jdf\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcollectToPython\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    818\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mlist\u001B[39m(_load_from_socket(sock_info, BatchedSerializer(CPickleSerializer())))\n",
      "File \u001B[0;32m~/.cache/pypoetry/virtualenvs/geoparquetandsedona-uqJLJxB7-py3.10/lib/python3.10/site-packages/py4j/java_gateway.py:1321\u001B[0m, in \u001B[0;36mJavaMember.__call__\u001B[0;34m(self, *args)\u001B[0m\n\u001B[1;32m   1315\u001B[0m command \u001B[38;5;241m=\u001B[39m proto\u001B[38;5;241m.\u001B[39mCALL_COMMAND_NAME \u001B[38;5;241m+\u001B[39m\\\n\u001B[1;32m   1316\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcommand_header \u001B[38;5;241m+\u001B[39m\\\n\u001B[1;32m   1317\u001B[0m     args_command \u001B[38;5;241m+\u001B[39m\\\n\u001B[1;32m   1318\u001B[0m     proto\u001B[38;5;241m.\u001B[39mEND_COMMAND_PART\n\u001B[1;32m   1320\u001B[0m answer \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mgateway_client\u001B[38;5;241m.\u001B[39msend_command(command)\n\u001B[0;32m-> 1321\u001B[0m return_value \u001B[38;5;241m=\u001B[39m \u001B[43mget_return_value\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m   1322\u001B[0m \u001B[43m    \u001B[49m\u001B[43manswer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mgateway_client\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtarget_id\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mname\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1324\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m temp_arg \u001B[38;5;129;01min\u001B[39;00m temp_args:\n\u001B[1;32m   1325\u001B[0m     temp_arg\u001B[38;5;241m.\u001B[39m_detach()\n",
      "File \u001B[0;32m~/.cache/pypoetry/virtualenvs/geoparquetandsedona-uqJLJxB7-py3.10/lib/python3.10/site-packages/pyspark/sql/utils.py:190\u001B[0m, in \u001B[0;36mcapture_sql_exception.<locals>.deco\u001B[0;34m(*a, **kw)\u001B[0m\n\u001B[1;32m    188\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mdeco\u001B[39m(\u001B[38;5;241m*\u001B[39ma: Any, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkw: Any) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Any:\n\u001B[1;32m    189\u001B[0m     \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m--> 190\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mf\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43ma\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkw\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    191\u001B[0m     \u001B[38;5;28;01mexcept\u001B[39;00m Py4JJavaError \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[1;32m    192\u001B[0m         converted \u001B[38;5;241m=\u001B[39m convert_exception(e\u001B[38;5;241m.\u001B[39mjava_exception)\n",
      "File \u001B[0;32m~/.cache/pypoetry/virtualenvs/geoparquetandsedona-uqJLJxB7-py3.10/lib/python3.10/site-packages/py4j/protocol.py:326\u001B[0m, in \u001B[0;36mget_return_value\u001B[0;34m(answer, gateway_client, target_id, name)\u001B[0m\n\u001B[1;32m    324\u001B[0m value \u001B[38;5;241m=\u001B[39m OUTPUT_CONVERTER[\u001B[38;5;28mtype\u001B[39m](answer[\u001B[38;5;241m2\u001B[39m:], gateway_client)\n\u001B[1;32m    325\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m answer[\u001B[38;5;241m1\u001B[39m] \u001B[38;5;241m==\u001B[39m REFERENCE_TYPE:\n\u001B[0;32m--> 326\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m Py4JJavaError(\n\u001B[1;32m    327\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mAn error occurred while calling \u001B[39m\u001B[38;5;132;01m{0}\u001B[39;00m\u001B[38;5;132;01m{1}\u001B[39;00m\u001B[38;5;132;01m{2}\u001B[39;00m\u001B[38;5;124m.\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;241m.\u001B[39m\n\u001B[1;32m    328\u001B[0m         \u001B[38;5;28mformat\u001B[39m(target_id, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m.\u001B[39m\u001B[38;5;124m\"\u001B[39m, name), value)\n\u001B[1;32m    329\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    330\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m Py4JError(\n\u001B[1;32m    331\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mAn error occurred while calling \u001B[39m\u001B[38;5;132;01m{0}\u001B[39;00m\u001B[38;5;132;01m{1}\u001B[39;00m\u001B[38;5;132;01m{2}\u001B[39;00m\u001B[38;5;124m. Trace:\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;132;01m{3}\u001B[39;00m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;241m.\u001B[39m\n\u001B[1;32m    332\u001B[0m         \u001B[38;5;28mformat\u001B[39m(target_id, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m.\u001B[39m\u001B[38;5;124m\"\u001B[39m, name, value))\n",
      "\u001B[0;31mPy4JJavaError\u001B[0m: An error occurred while calling o49.collectToPython.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 21.0 failed 1 times, most recent failure: Lost task 0.0 in stage 21.0 (TID 19) (10.50.2.80 executor driver): org.apache.spark.SparkException: Kryo serialization failed: Buffer overflow. Available: 0, required: 67108864. To avoid this, increase spark.kryoserializer.buffer.max value.\n\tat org.apache.spark.serializer.KryoSerializerInstance.serialize(KryoSerializer.scala:391)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:593)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n\tat java.base/java.lang.Thread.run(Thread.java:829)\nCaused by: com.esotericsoftware.kryo.KryoException: Buffer overflow. Available: 0, required: 67108864\n\tat com.esotericsoftware.kryo.io.Output.require(Output.java:167)\n\tat com.esotericsoftware.kryo.io.Output.writeBytes(Output.java:251)\n\tat com.esotericsoftware.kryo.io.Output.writeBytes(Output.java:237)\n\tat com.esotericsoftware.kryo.serializers.DefaultArraySerializers$ByteArraySerializer.write(DefaultArraySerializers.java:49)\n\tat com.esotericsoftware.kryo.serializers.DefaultArraySerializers$ByteArraySerializer.write(DefaultArraySerializers.java:38)\n\tat com.esotericsoftware.kryo.Kryo.writeClassAndObject(Kryo.java:651)\n\tat com.twitter.chill.Tuple2Serializer.write(TupleSerializers.scala:37)\n\tat com.twitter.chill.Tuple2Serializer.write(TupleSerializers.scala:33)\n\tat com.esotericsoftware.kryo.Kryo.writeClassAndObject(Kryo.java:651)\n\tat com.esotericsoftware.kryo.serializers.DefaultArraySerializers$ObjectArraySerializer.write(DefaultArraySerializers.java:361)\n\tat com.esotericsoftware.kryo.serializers.DefaultArraySerializers$ObjectArraySerializer.write(DefaultArraySerializers.java:302)\n\tat com.esotericsoftware.kryo.Kryo.writeClassAndObject(Kryo.java:651)\n\tat org.apache.spark.serializer.KryoSerializerInstance.serialize(KryoSerializer.scala:387)\n\t... 4 more\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2672)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2608)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2607)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2607)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1182)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1182)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1182)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2860)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2802)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2791)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:952)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2228)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2249)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2268)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2293)\n\tat org.apache.spark.rdd.RDD.$anonfun$collect$1(RDD.scala:1021)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:406)\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:1020)\n\tat org.apache.spark.sql.execution.SparkPlan.executeCollect(SparkPlan.scala:424)\n\tat org.apache.spark.sql.Dataset.$anonfun$collectToPython$1(Dataset.scala:3688)\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$2(Dataset.scala:3858)\n\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:510)\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:3856)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:109)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:169)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:95)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:779)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:3856)\n\tat org.apache.spark.sql.Dataset.collectToPython(Dataset.scala:3685)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:829)\nCaused by: org.apache.spark.SparkException: Kryo serialization failed: Buffer overflow. Available: 0, required: 67108864. To avoid this, increase spark.kryoserializer.buffer.max value.\n\tat org.apache.spark.serializer.KryoSerializerInstance.serialize(KryoSerializer.scala:391)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:593)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n\t... 1 more\nCaused by: com.esotericsoftware.kryo.KryoException: Buffer overflow. Available: 0, required: 67108864\n\tat com.esotericsoftware.kryo.io.Output.require(Output.java:167)\n\tat com.esotericsoftware.kryo.io.Output.writeBytes(Output.java:251)\n\tat com.esotericsoftware.kryo.io.Output.writeBytes(Output.java:237)\n\tat com.esotericsoftware.kryo.serializers.DefaultArraySerializers$ByteArraySerializer.write(DefaultArraySerializers.java:49)\n\tat com.esotericsoftware.kryo.serializers.DefaultArraySerializers$ByteArraySerializer.write(DefaultArraySerializers.java:38)\n\tat com.esotericsoftware.kryo.Kryo.writeClassAndObject(Kryo.java:651)\n\tat com.twitter.chill.Tuple2Serializer.write(TupleSerializers.scala:37)\n\tat com.twitter.chill.Tuple2Serializer.write(TupleSerializers.scala:33)\n\tat com.esotericsoftware.kryo.Kryo.writeClassAndObject(Kryo.java:651)\n\tat com.esotericsoftware.kryo.serializers.DefaultArraySerializers$ObjectArraySerializer.write(DefaultArraySerializers.java:361)\n\tat com.esotericsoftware.kryo.serializers.DefaultArraySerializers$ObjectArraySerializer.write(DefaultArraySerializers.java:302)\n\tat com.esotericsoftware.kryo.Kryo.writeClassAndObject(Kryo.java:651)\n\tat org.apache.spark.serializer.KryoSerializerInstance.serialize(KryoSerializer.scala:387)\n\t... 4 more\n"
     ]
    }
   ],
   "source": [
    "gdf = get_geopandas_df(fr_commune_df)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-15T14:03:16.401282159Z",
     "start_time": "2024-04-15T14:03:10.821930661Z"
    }
   },
   "id": "f1abd4b03fb11ee5"
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 28:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+-------+\n",
      "|            geometry|          properties|   type|\n",
      "+--------------------+--------------------+-------+\n",
      "|{[121.4961111, 25...|{Yungho, yungho, ...|Feature|\n",
      "|{[-72.233333, -37...|{Mulchen, mulchen...|Feature|\n",
      "|{[-73.6405556, 40...|{Oceanside, ocean...|Feature|\n",
      "|{[-70.966667, -32...|{Llaillay, llaill...|Feature|\n",
      "|{[35.6, 3.1166667...|{Lodwar, lodwar, ...|Feature|\n",
      "|{[10.1666667, 5.9...|{Bamenda, bamenda...|Feature|\n",
      "|{[-45.533333, -20...|{Arcos, arcos, br...|Feature|\n",
      "|{[-43.716944, -22...|{Seropédica, sero...|Feature|\n",
      "|{[-97.1413889, 32...|{Mansfield, mansf...|Feature|\n",
      "|{[-67.5419444, 10...|{Palo Negro, palo...|Feature|\n",
      "|{[-42.683333, -5....|{Demerval Lobão, ...|Feature|\n",
      "|{[-48.666667, -28...|{Imbituba, imbitu...|Feature|\n",
      "|{[-49.333333, -5....|{Itupiranga, itup...|Feature|\n",
      "|{[121.75, 24.7666...|{Ilan, ilan, tw, ...|Feature|\n",
      "|{[49.1825, 11.284...|{Bosaso, bosaso, ...|Feature|\n",
      "|{[64.570048, 31.8...|{Geresk, geresk, ...|Feature|\n",
      "|{[7.573271, 47.55...|{Basel, basel, ch...|Feature|\n",
      "|{[76.099444, 13.0...|{Hassan, hassan, ...|Feature|\n",
      "|{[17.637616, 47.6...|{Gyor, gyor, hu, ...|Feature|\n",
      "|{[6.92917, 43.660...|{Grasse, grasse, ...|Feature|\n",
      "+--------------------+--------------------+-------+\n",
      "only showing top 20 rows\n",
      "\n",
      "root\n",
      " |-- geometry: struct (nullable = true)\n",
      " |    |-- coordinates: array (nullable = true)\n",
      " |    |    |-- element: double (containsNull = true)\n",
      " |    |-- type: string (nullable = true)\n",
      " |-- properties: struct (nullable = true)\n",
      " |    |-- accentcity: string (nullable = true)\n",
      " |    |-- city: string (nullable = true)\n",
      " |    |-- country: string (nullable = true)\n",
      " |    |-- geopoint: array (nullable = true)\n",
      " |    |    |-- element: double (containsNull = true)\n",
      " |    |-- latitude: double (nullable = true)\n",
      " |    |-- longitude: double (nullable = true)\n",
      " |    |-- population: long (nullable = true)\n",
      " |    |-- region: string (nullable = true)\n",
      " |-- type: string (nullable = true)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "city_geoj_file_path = \"/home/pengfei/data_set/kaggle/geospatial/world-cities.geojson\"\n",
    "\n",
    "# read geo json\n",
    "# the selectExpr action Explode the envelope to get one feature per row.\n",
    "#  Unpack the features' struct. \n",
    "df = sedona.read.format(\"json\").option(\"multiLine\", \"true\").load(city_geoj_file_path) \\\n",
    " .selectExpr(\"explode(features) as features\")  \\\n",
    " .select(\"features.*\")  \n",
    " # .withColumn(\"prop0\", f.expr(\"properties['prop0']\")).drop(\"properties\").drop(\"type\")\n",
    "\n",
    "df.show()\n",
    "df.printSchema()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-15T14:35:25.513712656Z",
     "start_time": "2024-04-15T14:35:21.051507270Z"
    }
   },
   "id": "4d3fb1626fdcb953"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "1c0246b76fba9544"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
