{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Use sedona to read various geospatial data format\n",
    "\n",
    "To work with geospatial data, it's essential to read and write data in geospatial data format. The **full list of the constructor for the geo data types** can be found [here](https://sedona.apache.org/1.6.1/api/sql/Constructor/)\n",
    "For sedona version `1.6.1`, it supports at least 10 formats:\n",
    "\n",
    "In this tutorial, we will show example of sedona to read various geospatial data format such as:\n",
    "- geojson\n",
    "- shape file\n",
    "- csv/tsv\n",
    "- pbf\n",
    "- geoparquet\n",
    "\n",
    "## Geospatial data \n",
    "\n",
    "To be able to use sedona to do geospatial operations (e.g calculate distance, area hierarchy, etc.), we need to construct geo dataframe first. A geo dataframe contains one or more columns of below type:\n",
    "- Point : a point on the map with a (x,y) coordinates\n",
    "- Line: two point which can form a line\n",
    "- Polygon: a list of point which can form a polygon\n",
    "\n",
    "The `geometry column` must be able to express these data types.\n",
    "\n",
    "\n",
    "\n",
    "We will also evaluate the performance(e.g. storage space, processing speed) of each format"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "2e0837b4717cb931"
  },
  {
   "cell_type": "code",
   "source": [
    "from sedona.spark import *\n",
    "import geopandas as gpd\n",
    "from pyspark.sql.functions import trim, col\n",
    "from pathlib import Path\n",
    "import json\n",
    "\n",
    "from ipyleaflet import Map, basemaps, basemap_to_tiles, MarkerCluster, Marker, AwesomeIcon\n",
    "from ipywidgets import Layout\n",
    "import numpy as np"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-11-27T08:10:54.501495Z",
     "start_time": "2024-11-27T08:10:50.838574Z"
    }
   },
   "id": "ce7c87816a39c1a6",
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-27T08:10:56.411162Z",
     "start_time": "2024-11-27T08:10:56.395522Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# get the project root dir\n",
    "project_root_dir = Path.cwd().parent.parent\n",
    "data_dir = f\"{project_root_dir}/data\""
   ],
   "id": "336ea98726722af5",
   "outputs": [],
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "source": [
    "# build a sedona session (sedona = 1.6.1)\n",
    "jar_folder = Path(f\"{project_root_dir}/jars/sedona-35-213-161\")\n",
    "jar_list = [str(jar) for jar in jar_folder.iterdir() if jar.is_file()]\n",
    "jar_path = \",\".join(jar_list)\n",
    "\n",
    "# build a sedona session (sedona = 1.6.1) offline\n",
    "config = SedonaContext.builder() \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .config('spark.jars', jar_path). \\\n",
    "    getOrCreate()\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-11-27T08:11:27.292082Z",
     "start_time": "2024-11-27T08:10:59.827479Z"
    }
   },
   "id": "7b1a2689b9abef60",
   "outputs": [],
   "execution_count": 3
  },
  {
   "cell_type": "code",
   "source": [
    "# create a sedona context\n",
    "sedona = SedonaContext.create(config)\n",
    "sc = sedona.sparkContext\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-11-27T08:16:49.280334Z",
     "start_time": "2024-11-27T08:16:43.649617Z"
    }
   },
   "id": "ec2dc3be27ed3c86",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-27T08:16:50.940076Z",
     "start_time": "2024-11-27T08:16:50.920995Z"
    }
   },
   "cell_type": "code",
   "source": "spark = sedona.getActiveSession()",
   "id": "ab5399a11fd94a6b",
   "outputs": [],
   "execution_count": 5
  },
  {
   "cell_type": "code",
   "source": [
    "# this sets the encoding of shape files\n",
    "sc.setSystemProperty(\"sedona.global.charset\", \"utf8\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-11-27T08:16:52.485218Z",
     "start_time": "2024-11-27T08:16:52.473319Z"
    }
   },
   "id": "cb60751167bf9567",
   "outputs": [],
   "execution_count": 6
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 1. Read from plain text string\n",
    "\n",
    "Sedona can read various plain text geospatial data format\n",
    "- CSV/TSV\n",
    "- wkt/wkb (Ewkt/Ekb)\n",
    "- geojson\n",
    "\n",
    "In below example, we will read a normal csv file which contains two column x, y. You can notice the content of the csv is `plain text` string.\n",
    "\n",
    "### 1.1 Point example\n",
    "\n",
    "In below example, we will construct a geo dataframe which contains a **Point** column"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "da4e63a4a87b99cb"
  },
  {
   "cell_type": "code",
   "source": [
    "point_file_path = f\"{data_dir}/csv/test_points.csv\"\n",
    "\n",
    "# read a normal csv\n",
    "raw_point_df = sedona.read.format(\"csv\").\\\n",
    "          option(\"delimiter\",\",\").\\\n",
    "          option(\"header\",\"false\").\\\n",
    "          load(point_file_path)\n",
    "\n",
    "raw_point_df.printSchema()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-11-27T08:16:55.814836Z",
     "start_time": "2024-11-27T08:16:54.659530Z"
    }
   },
   "id": "e4172408db2d7a3b",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- _c0: string (nullable = true)\n",
      " |-- _c1: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-27T08:16:59.096780Z",
     "start_time": "2024-11-27T08:16:58.880044Z"
    }
   },
   "cell_type": "code",
   "source": "raw_point_df.show(5)",
   "id": "fa4a13e912e462e6",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+\n",
      "|_c0|  _c1|\n",
      "+---+-----+\n",
      "|1.1|101.1|\n",
      "|2.1|102.1|\n",
      "|3.1|103.1|\n",
      "|4.1|104.1|\n",
      "|5.1|105.1|\n",
      "+---+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-27T08:17:06.988147Z",
     "start_time": "2024-11-27T08:17:06.936977Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# create a temp view\n",
    "raw_point_df.createOrReplaceTempView(\"p_raw_table\")"
   ],
   "id": "f9a5233b8b8a2479",
   "outputs": [],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-27T08:17:09.021767Z",
     "start_time": "2024-11-27T08:17:08.825179Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# we cast the string type to decimal first, then we use `ST_Point` function to build a geometry column by using the two column in the csv file \n",
    "geo_point_df = sedona.sql(\"select ST_Point(cast(p_raw_table._c0 as Decimal(24,20)), cast(p_raw_table._c1 as Decimal(24,20))) as point from p_raw_table\")\n",
    "geo_point_df.printSchema()"
   ],
   "id": "cabdfd0c30df8030",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- point: geometry (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-27T08:17:11.800050Z",
     "start_time": "2024-11-27T08:17:11.537024Z"
    }
   },
   "cell_type": "code",
   "source": "geo_point_df.show(5,truncate=False)",
   "id": "3cac69b21d65b710",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+\n",
      "|point            |\n",
      "+-----------------+\n",
      "|POINT (1.1 101.1)|\n",
      "|POINT (2.1 102.1)|\n",
      "|POINT (3.1 103.1)|\n",
      "|POINT (4.1 104.1)|\n",
      "|POINT (5.1 105.1)|\n",
      "+-----------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "execution_count": 11
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 1.2 Line example\n",
    "\n",
    "To create a line type, we can use the constructor **ST_LineStringFromText (Text:string, Delimiter:char)**. In below example, we can notice it takes a list of gps coordinates, then it returns a geometry column."
   ],
   "id": "7c1fa9fa2dd30d8e"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-27T08:17:15.275515Z",
     "start_time": "2024-11-27T08:17:15.244272Z"
    }
   },
   "cell_type": "code",
   "source": [
    "geo_line_df = sedona.sql(\"SELECT ST_LineStringFromText('-74.0428197,40.6867969,-74.0421975,40.6921336,-74.0508020,40.6912794', ',') AS line\")\n",
    "geo_line_df.printSchema()"
   ],
   "id": "bcd341497adad0f0",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- line: geometry (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-27T08:17:20.831370Z",
     "start_time": "2024-11-27T08:17:20.744924Z"
    }
   },
   "cell_type": "code",
   "source": "geo_line_df.show(5,truncate=False)",
   "id": "98e60682eceae1dd",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------------------------------------------------------------------+\n",
      "|line                                                                              |\n",
      "+----------------------------------------------------------------------------------+\n",
      "|LINESTRING (-74.0428197 40.6867969, -74.0421975 40.6921336, -74.050802 40.6912794)|\n",
      "+----------------------------------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "execution_count": 13
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 1.3 Polygon example\n",
    "\n",
    "We have seen the below example for the section 1. We will use the constructor **ST_GeomFromText()**"
   ],
   "id": "a6d450d5a1a36e35"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-25T15:05:00.812060Z",
     "start_time": "2024-11-25T15:05:00.792168Z"
    }
   },
   "cell_type": "code",
   "source": "us_county_file_path = f\"{data_dir}/csv/county_small.tsv\"",
   "id": "183d62c1544c3d35",
   "outputs": [],
   "execution_count": 23
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-25T15:10:56.900427Z",
     "start_time": "2024-11-25T15:10:56.328861Z"
    }
   },
   "cell_type": "code",
   "source": [
    "raw_df = sedona.read.format(\"csv\").option(\"delimiter\", \"\\t\").option(\"header\", \"false\").load(us_county_file_path)\n",
    "raw_df.show()"
   ],
   "id": "5e69b0ae1776622f",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+---+---+--------+-----+-----------+--------------------+---+---+-----+----+-----+----+----+----------+--------+-----------+------------+\n",
      "|                 _c0|_c1|_c2|     _c3|  _c4|        _c5|                 _c6|_c7|_c8|  _c9|_c10| _c11|_c12|_c13|      _c14|    _c15|       _c16|        _c17|\n",
      "+--------------------+---+---+--------+-----+-----------+--------------------+---+---+-----+----+-----+----+----+----------+--------+-----------+------------+\n",
      "|POLYGON ((-97.019...| 31|039|00835841|31039|     Cuming|       Cuming County| 06| H1|G4020|NULL| NULL|NULL|   A|1477895811|10447360|+41.9158651|-096.7885168|\n",
      "|POLYGON ((-123.43...| 53|069|01513275|53069|  Wahkiakum|    Wahkiakum County| 06| H1|G4020|NULL| NULL|NULL|   A| 682138871|61658258|+46.2946377|-123.4244583|\n",
      "|POLYGON ((-104.56...| 35|011|00933054|35011|    De Baca|      De Baca County| 06| H1|G4020|NULL| NULL|NULL|   A|6015539696|29159492|+34.3592729|-104.3686961|\n",
      "|POLYGON ((-96.910...| 31|109|00835876|31109|  Lancaster|    Lancaster County| 06| H1|G4020| 339|30700|NULL|   A|2169240202|22877180|+40.7835474|-096.6886584|\n",
      "|POLYGON ((-98.273...| 31|129|00835886|31129|   Nuckolls|     Nuckolls County| 06| H1|G4020|NULL| NULL|NULL|   A|1489645187| 1718484|+40.1764918|-098.0468422|\n",
      "|POLYGON ((-65.910...| 72|085|01804523|72085|Las Piedras|Las Piedras Munic...| 13| H1|G4020| 490|41980|NULL|   A|  87748363|   32509|+18.1871483|-065.8711890|\n",
      "|POLYGON ((-97.129...| 46|099|01265772|46099|  Minnehaha|    Minnehaha County| 06| H1|G4020|NULL|43620|NULL|   A|2090540341|17349847|+43.6674723|-096.7957261|\n",
      "|POLYGON ((-99.821...| 48|327|01383949|48327|     Menard|       Menard County| 06| H1|G4020|NULL| NULL|NULL|   A|2336245914|  613559|+30.8843655|-099.8539896|\n",
      "|POLYGON ((-120.65...| 06|091|00277310|06091|     Sierra|       Sierra County| 06| H1|G4020|NULL| NULL|NULL|   A|2468686374|23299110|+39.5769252|-120.5219926|\n",
      "|POLYGON ((-85.239...| 21|053|00516873|21053|    Clinton|      Clinton County| 06| H1|G4020|NULL| NULL|NULL|   A| 510864252|21164150|+36.7288647|-085.1534262|\n",
      "|POLYGON ((-83.880...| 39|063|01074044|39063|    Hancock|      Hancock County| 06| H1|G4020| 248|22300|NULL|   A|1376210232| 5959837|+41.0004711|-083.6660335|\n",
      "|POLYGON ((-102.08...| 48|189|01383880|48189|       Hale|         Hale County| 06| H1|G4020|NULL|38380|NULL|   A|2602115649|  246678|+34.0684364|-101.8228879|\n",
      "|POLYGON ((-85.978...| 01|027|00161539|01027|       Clay|         Clay County| 06| H1|G4020|NULL| NULL|NULL|   A|1564252367| 5284573|+33.2703999|-085.8635254|\n",
      "|POLYGON ((-101.62...| 48|011|01383791|48011|  Armstrong|    Armstrong County| 06| H1|G4020| 108|11100|NULL|   A|2354581764|12219587|+34.9641790|-101.3566363|\n",
      "|POLYGON ((-84.397...| 39|003|01074015|39003|      Allen|        Allen County| 06| H1|G4020| 338|30620|NULL|   A|1042470093|11266164|+40.7716274|-084.1061032|\n",
      "|POLYGON ((-82.449...| 13|189|00348794|13189|   McDuffie|     McDuffie County| 06| H1|G4020|NULL|12260|NULL|   A| 666816637|23116292|+33.4824637|-082.4731880|\n",
      "|POLYGON ((-90.191...| 55|111|01581115|55111|       Sauk|         Sauk County| 06| H1|G4020| 357|12660|NULL|   A|2152007753|45296336|+43.4279976|-089.9433290|\n",
      "|POLYGON ((-92.415...| 05|137|00069902|05137|      Stone|        Stone County| 06| H1|G4020|NULL| NULL|NULL|   A|1570579427| 7841929|+35.8570312|-092.1405728|\n",
      "|POLYGON ((-117.74...| 41|063|01155135|41063|    Wallowa|      Wallowa County| 06| H1|G4020|NULL| NULL|NULL|   A|8148602810|14199330|+45.5937530|-117.1855796|\n",
      "|POLYGON ((-80.518...| 42|007|01214112|42007|     Beaver|       Beaver County| 06| H1|G4020| 430|38300|NULL|   A|1125901160|24165972|+40.6841401|-080.3507209|\n",
      "+--------------------+---+---+--------+-----+-----------+--------------------+---+---+-----+----+-----+----+----+----------+--------+-----------+------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "execution_count": 26
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-25T15:12:37.411879Z",
     "start_time": "2024-11-25T15:12:37.286113Z"
    }
   },
   "cell_type": "code",
   "source": [
    "raw_poly_df = raw_df.select(\"_c0\",\"_c6\").withColumnRenamed(\"_c0\",\"county_polygon\").withColumnRenamed(\"_c6\",\"county_name\")\n",
    "raw_poly_df.createOrReplaceTempView(\"gon_raw_table\")\n",
    "raw_poly_df.show(5)"
   ],
   "id": "a6b6b8c09e4ffd19",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+----------------+\n",
      "|      county_polygon|     county_name|\n",
      "+--------------------+----------------+\n",
      "|POLYGON ((-97.019...|   Cuming County|\n",
      "|POLYGON ((-123.43...|Wahkiakum County|\n",
      "|POLYGON ((-104.56...|  De Baca County|\n",
      "|POLYGON ((-96.910...|Lancaster County|\n",
      "|POLYGON ((-98.273...| Nuckolls County|\n",
      "+--------------------+----------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "execution_count": 30
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-25T15:11:38.598963Z",
     "start_time": "2024-11-25T15:11:38.591782Z"
    }
   },
   "cell_type": "code",
   "source": "raw_poly_df.printSchema()",
   "id": "da9bfcf4c4c13319",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- county_polygon: string (nullable = true)\n",
      " |-- county_name: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "execution_count": 29
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-25T15:13:48.666454Z",
     "start_time": "2024-11-25T15:13:48.370511Z"
    }
   },
   "cell_type": "code",
   "source": [
    "geo_polygon_df=sedona.sql(\"select ST_GeomFromText(gon_raw_table.county_polygon) as county_shape, gon_raw_table.county_name from gon_raw_table\")\n",
    "geo_polygon_df.show(5)"
   ],
   "id": "fe729421fdbf6673",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+----------------+\n",
      "|        county_shape|     county_name|\n",
      "+--------------------+----------------+\n",
      "|POLYGON ((-97.019...|   Cuming County|\n",
      "|POLYGON ((-123.43...|Wahkiakum County|\n",
      "|POLYGON ((-104.56...|  De Baca County|\n",
      "|POLYGON ((-96.910...|Lancaster County|\n",
      "|POLYGON ((-98.273...| Nuckolls County|\n",
      "+--------------------+----------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "execution_count": 32
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-25T15:14:11.804696Z",
     "start_time": "2024-11-25T15:14:11.794454Z"
    }
   },
   "cell_type": "code",
   "source": "geo_polygon_df.printSchema()",
   "id": "8f344d1715bcf7ca",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- county_shape: geometry (nullable = true)\n",
      " |-- county_name: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "execution_count": 33
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 1.4 Read wkt and wkb file\n",
    "\n",
    "**Well-known text (WKT)** is a `text markup language` for representing vector geometry objects on a map and spatial reference systems of spatial objects. A binary equivalent, known as **well-known binary (WKB)** is used to transfer and store the same information for geometry objects.\n",
    "\n",
    "Geometries in a `WKT and WKB` file always occupy a single column no matter how many coordinates they have. Sedona provides `WktReader and WkbReader` to create generic SpatialRDD. Then we need to convert the spatial rdd to dataframe.\n",
    "\n",
    "> You must use the wkt reader to read wkt file, and wkb reader to read wkb file.\n",
    "\n",
    "For `EWKT/EWKB`, we just have a extra column `SRID(Spatial Reference Identifier)  code` compare to WKT\n",
    "\n",
    "```sql\n",
    "SELECT ST_AsText(ST_GeomFromEWKT('SRID=4269;POINT(40.7128 -74.0060)'))\n",
    "\n",
    "# output example\n",
    "# POINT(40.7128 -74.006)\n",
    "```"
   ],
   "id": "74b3a3f54853084d"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-25T15:18:44.640722Z",
     "start_time": "2024-11-25T15:18:44.630550Z"
    }
   },
   "cell_type": "code",
   "source": "us_county_wkb_file_path = f\"{data_dir}/csv/county_small_wkb.tsv\"",
   "id": "53e07a59a59a3561",
   "outputs": [],
   "execution_count": 34
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-25T15:18:51.832206Z",
     "start_time": "2024-11-25T15:18:51.823144Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from sedona.core.formatMapper import WkbReader\n",
    "from sedona.utils.adapter import Adapter"
   ],
   "id": "9fbe8e171caaa444",
   "outputs": [],
   "execution_count": 35
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-25T15:20:49.383280Z",
     "start_time": "2024-11-25T15:20:49.288704Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# The WKT string starts from Column 0\n",
    "wkbColumn = 0 \n",
    "allowTopologyInvalidGeometries = True\n",
    "skipSyntaxInvalidGeometries = False\n",
    "\n",
    "spatialRdd = WkbReader.readToGeometryRDD(sedona.sparkContext, us_county_wkb_file_path, wkbColumn, allowTopologyInvalidGeometries, skipSyntaxInvalidGeometries)"
   ],
   "id": "96433ef189ab7880",
   "outputs": [],
   "execution_count": 36
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-25T15:21:57.970716Z",
     "start_time": "2024-11-25T15:21:57.870097Z"
    }
   },
   "cell_type": "code",
   "source": [
    "geo_county_wkb_df = Adapter.toDf(spatialRdd,sedona).withColumnRenamed(\"geometry\", \"county_shape\")\n",
    "geo_county_wkb_df.show(5)"
   ],
   "id": "1c6c6780175b336b",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|        county_shape|\n",
      "+--------------------+\n",
      "|POLYGON ((-97.019...|\n",
      "|POLYGON ((-123.43...|\n",
      "|POLYGON ((-104.56...|\n",
      "|POLYGON ((-96.910...|\n",
      "|POLYGON ((-98.273...|\n",
      "+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "execution_count": 40
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-25T15:22:01.497812Z",
     "start_time": "2024-11-25T15:22:01.482883Z"
    }
   },
   "cell_type": "code",
   "source": "geo_county_wkb_df.printSchema()",
   "id": "651a69c24a719aa0",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- county_shape: geometry (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "execution_count": 41
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 1.5 Read geojson\n",
    "\n",
    "https://sedona.apache.org/1.6.1/tutorial/sql\n",
    "Geojson has two different organization:\n",
    "- single-line GeoJSON\n",
    "- multi-line \n",
    "\n",
    "#### 1.5.1 Read single-line GeoJSON\n",
    "\n",
    "In the single-line geoJSON organization, each line is a separate, self-contained GeoJSON object. Below is an example\n",
    "\n",
    "```json\n",
    "{\"type\":\"Feature\",\"geometry\":{\"type\":\"Point\",\"coordinates\":[102.0,0.5]},\"properties\":{\"prop0\":\"value0\"}}\n",
    "{\"type\":\"Feature\",\"geometry\":{\"type\":\"LineString\",\"coordinates\":[[102.0,0.0],[103.0,1.0],[104.0,0.0],[105.0,1.0]]},\"properties\":{\"prop0\":\"value1\"}}\n",
    "{\"type\":\"Feature\",\"geometry\":{\"type\":\"Polygon\",\"coordinates\":[[[100.0,0.0],[101.0,0.0],[101.0,1.0],[100.0,1.0],[100.0,0.0]]]},\"properties\":{\"prop0\":\"value2\"}}\n",
    "```\n",
    "You can notice that each line starts with `{` ends with `}`, which means it's a self-contained json object.\n",
    "\n",
    "> This format is efficient for processing large datasets, because each line is an independent GeoJSON Feature which can be processed in parallel.  "
   ],
   "id": "a80ffc027087658"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-27T08:17:54.729850Z",
     "start_time": "2024-11-27T08:17:54.719375Z"
    }
   },
   "cell_type": "code",
   "source": "us_county_json_file_path = f\"{data_dir}/geojson/us_county.json\"",
   "id": "382d006942380f41",
   "outputs": [],
   "execution_count": 14
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-27T08:17:57.628008Z",
     "start_time": "2024-11-27T08:17:56.838571Z"
    }
   },
   "cell_type": "code",
   "source": [
    "raw_json_df = sedona.read.format(\"geojson\").load(us_county_json_file_path)\n",
    "raw_json_df.show(5)"
   ],
   "id": "4f009857a4c8d3c7",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+-------+\n",
      "|            geometry|          properties|   type|\n",
      "+--------------------+--------------------+-------+\n",
      "|POLYGON ((-87.621...|{1500000US0107701...|Feature|\n",
      "|POLYGON ((-85.719...|{1500000US0104502...|Feature|\n",
      "|POLYGON ((-86.000...|{1500000US0105500...|Feature|\n",
      "|POLYGON ((-86.574...|{1500000US0108900...|Feature|\n",
      "|POLYGON ((-85.382...|{1500000US0106904...|Feature|\n",
      "+--------------------+--------------------+-------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "execution_count": 15
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-27T08:17:59.957974Z",
     "start_time": "2024-11-27T08:17:59.943239Z"
    }
   },
   "cell_type": "code",
   "source": "raw_json_df.printSchema()",
   "id": "78bf74e51113c324",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- geometry: geometry (nullable = true)\n",
      " |-- properties: struct (nullable = true)\n",
      " |    |-- AFFGEOID: string (nullable = true)\n",
      " |    |-- ALAND: long (nullable = true)\n",
      " |    |-- AWATER: long (nullable = true)\n",
      " |    |-- BLKGRPCE: string (nullable = true)\n",
      " |    |-- COUNTYFP: string (nullable = true)\n",
      " |    |-- GEOID: string (nullable = true)\n",
      " |    |-- LSAD: string (nullable = true)\n",
      " |    |-- NAME: string (nullable = true)\n",
      " |    |-- STATEFP: string (nullable = true)\n",
      " |    |-- TRACTCE: string (nullable = true)\n",
      " |-- type: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "execution_count": 16
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 1.5.2 Read multi-line GeoJSON\n",
    "\n",
    "The multi-line GeoJSON use a global `{ \"type\": \"FeatureCollection\", }` to encapsulate all geo features in one JSON object. Below is an example\n",
    "\n",
    "```json\n",
    "{ \"type\": \"FeatureCollection\",\n",
    "    \"features\": [\n",
    "      { \"type\": \"Feature\",\n",
    "        \"geometry\": {\"type\": \"Point\", \"coordinates\": [102.0, 0.5]},\n",
    "        \"properties\": {\"prop0\": \"value0\"}\n",
    "        },\n",
    "      { \"type\": \"Feature\",\n",
    "        \"geometry\": {\n",
    "          \"type\": \"LineString\",\n",
    "          \"coordinates\": [\n",
    "            [102.0, 0.0], [103.0, 1.0], [104.0, 0.0], [105.0, 1.0]\n",
    "            ]\n",
    "          },\n",
    "        \"properties\": {\n",
    "          \"prop0\": \"value1\",\n",
    "          \"prop1\": 0.0\n",
    "          }\n",
    "        },\n",
    "      { \"type\": \"Feature\",\n",
    "         \"geometry\": {\n",
    "           \"type\": \"Polygon\",\n",
    "           \"coordinates\": [\n",
    "             [ [100.0, 0.0], [101.0, 0.0], [101.0, 1.0],\n",
    "               [100.0, 1.0], [100.0, 0.0] ]\n",
    "             ]\n",
    "         },\n",
    "         \"properties\": {\n",
    "           \"prop0\": \"value2\",\n",
    "           \"prop1\": {\"this\": \"that\"}\n",
    "           }\n",
    "         }\n",
    "       ]\n",
    "}\n",
    "```\n",
    "\n",
    "Multiline format is preferable for scenarios where files need to be human-readable or manually edited.\n",
    "\n",
    "As the entire file is considered as a single json object, it's hard to process in parallel"
   ],
   "id": "791d73200fd0eecc"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-27T08:42:34.806071Z",
     "start_time": "2024-11-27T08:42:34.642251Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from pyspark.sql.functions import expr\n",
    "\n",
    "multi_line_json_file_path = f\"{data_dir}/geojson/multi_lines.json\"\n",
    "\n",
    "df_raw = sedona.read.format(\"geojson\").option(\"multiLine\", \"true\").load(multi_line_json_file_path)\n",
    "          \n",
    "df_raw.show(5,truncate=False)\n",
    "df_raw.printSchema()"
   ],
   "id": "883de592ef21f585",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+-----------------+\n",
      "|features                                                                                                                                                                                            |type             |\n",
      "+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+-----------------+\n",
      "|[{POINT (102 0.5), {value0, NULL}, Feature}, {LINESTRING (102 0, 103 1, 104 0, 105 1), {value1, 0.0}, Feature}, {POLYGON ((100 0, 101 0, 101 1, 100 1, 100 0)), {value2, {\"this\":\"that\"}}, Feature}]|FeatureCollection|\n",
      "+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+-----------------+\n",
      "\n",
      "root\n",
      " |-- features: array (nullable = true)\n",
      " |    |-- element: struct (containsNull = true)\n",
      " |    |    |-- geometry: geometry (nullable = true)\n",
      " |    |    |-- properties: struct (nullable = true)\n",
      " |    |    |    |-- prop0: string (nullable = true)\n",
      " |    |    |    |-- prop1: string (nullable = true)\n",
      " |    |    |-- type: string (nullable = true)\n",
      " |-- type: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "execution_count": 24
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "As the entire file is a single json object(FeatureCollection), all features are consider as items in one big array, To get one feature per row, we need to explode the array envelope.",
   "id": "3d5499209a2899c3"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-27T08:38:33.052386Z",
     "start_time": "2024-11-27T08:38:32.934913Z"
    }
   },
   "cell_type": "code",
   "source": [
    "df = df_raw.selectExpr(\"explode(features) as features\").select(\"features.*\")\n",
    " \n",
    "\n",
    "df.show()\n",
    "df.printSchema()"
   ],
   "id": "9dbb04ab4d951435",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+-------+\n",
      "|            geometry|          properties|   type|\n",
      "+--------------------+--------------------+-------+\n",
      "|     POINT (102 0.5)|      {value0, NULL}|Feature|\n",
      "|LINESTRING (102 0...|       {value1, 0.0}|Feature|\n",
      "|POLYGON ((100 0, ...|{value2, {\"this\":...|Feature|\n",
      "+--------------------+--------------------+-------+\n",
      "\n",
      "root\n",
      " |-- geometry: geometry (nullable = true)\n",
      " |-- properties: struct (nullable = true)\n",
      " |    |-- prop0: string (nullable = true)\n",
      " |    |-- prop1: string (nullable = true)\n",
      " |-- type: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "execution_count": 23
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 1.6 Read GML\n",
    "\n",
    "**GML(Geography Markup Language)** is an `XML based encoding standard` for geographic information developed by the `OpenGIS Consortium (OGC)`. You can find the official doc [here](https://www.ogc.org/publications/standard/gml/)\n",
    "It has three major version:\n",
    "- GML 1\n",
    "- GML 2\n",
    "- GML 3\n",
    "\n",
    "Sedona(<v1.6.1) only supports `GML1 and GML2` for now.  "
   ],
   "id": "85b28992d1387238"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-27T09:37:19.269859Z",
     "start_time": "2024-11-27T09:37:19.248576Z"
    }
   },
   "cell_type": "code",
   "source": [
    "gml_sample = \"\"\"\n",
    "<gml:LineString srsName=\"EPSG:4269\">\n",
    "        <gml:coordinates>\n",
    "            -71.16028,42.258729\n",
    "            -71.160837,42.259112\n",
    "            -71.161143,42.25932\n",
    "        </gml:coordinates>\n",
    "</gml:LineString>\n",
    "\"\"\"\n",
    "\n",
    "gml_df = sedona.sql(f\"SELECT ST_GeomFromGML('{gml_sample}') as line\")"
   ],
   "id": "2cfb962363025ccd",
   "outputs": [],
   "execution_count": 33
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-27T09:37:32.841357Z",
     "start_time": "2024-11-27T09:37:32.757931Z"
    }
   },
   "cell_type": "code",
   "source": [
    "gml_df.show(5, truncate=False)\n",
    "gml_df.printSchema()"
   ],
   "id": "f288f5801e68ef26",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------------------------------------------------------------+\n",
      "|line                                                                       |\n",
      "+---------------------------------------------------------------------------+\n",
      "|LINESTRING (-71.16028 42.258729, -71.160837 42.259112, -71.161143 42.25932)|\n",
      "+---------------------------------------------------------------------------+\n",
      "\n",
      "root\n",
      " |-- line: geometry (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "execution_count": 35
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 1.7 Read KML\n",
    "\n",
    "**Keyhole Markup Language (KML)** is an `XML notation` for expressing geographic annotation and visualization within two-dimensional maps and three-dimensional Earth browsers. `KML was developed for use with Google Earth`, which was originally named `Keyhole Earth Viewer`.\n",
    "\n",
    "A complete kml file example.\n",
    "\n",
    "```xml\n",
    "<?xml version=\"1.0\" encoding=\"UTF-8\"?>\n",
    "<kml xmlns=\"http://www.opengis.net/kml/2.2\">\n",
    "<Document>\n",
    "<Placemark>\n",
    "  <name>New York City</name>\n",
    "  <description>New York City</description>\n",
    "  <Point>\n",
    "    <coordinates>-74.006393,40.714172,0</coordinates>\n",
    "  </Point>\n",
    "</Placemark>\n",
    "</Document>\n",
    "</kml>\n",
    "``` \n",
    "\n",
    "You can notice the coordinates has three attributes (longitude,latitude,altitude)\n",
    "\n",
    "\n",
    "\n"
   ],
   "id": "2aba54555eae2c36"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Disk usage\n",
    "\n",
    "The shape file use 301 MB disk space"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "507549fa81351e8"
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8.9M\t/home/pengfei/data_set/kaggle/geospatial/communes_fr_shape/communes-20220101.dbf\r\n",
      "4.0K\t/home/pengfei/data_set/kaggle/geospatial/communes_fr_shape/communes-20220101.prj\r\n",
      "4.0K\t/home/pengfei/data_set/kaggle/geospatial/communes_fr_shape/communes-descriptif.txt\r\n",
      "276K\t/home/pengfei/data_set/kaggle/geospatial/communes_fr_shape/communes-20220101.shx\r\n",
      "4.0K\t/home/pengfei/data_set/kaggle/geospatial/communes_fr_shape/LICENCE.txt\r\n",
      "4.0K\t/home/pengfei/data_set/kaggle/geospatial/communes_fr_shape/communes-20220101.cpg\r\n",
      "292M\t/home/pengfei/data_set/kaggle/geospatial/communes_fr_shape/communes-20220101.shp\r\n",
      "301M\t/home/pengfei/data_set/kaggle/geospatial/communes_fr_shape\r\n"
     ]
    }
   ],
   "source": [
    "! du -ah /home/pengfei/data_set/kaggle/geospatial/communes_fr_shape\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-15T14:27:35.598403879Z",
     "start_time": "2024-04-15T14:27:34.466755621Z"
    }
   },
   "id": "d90a99b5ae12e00e"
  },
  {
   "cell_type": "code",
   "source": [
    "from pyspark.sql import DataFrame\n",
    "\n",
    "\n",
    "def get_nearest_commune(df:DataFrame, latitude:str, longitude:str, max_commune_number:int):\n",
    "    temp_table_name:str = \"temp_tab\"\n",
    "    df.createOrReplaceTempView(temp_table_name)\n",
    "    nearest_commune_df = sedona.sql(f\"\"\"\n",
    "     SELECT z.nom as commune_name, z.insee, ST_DistanceSphere(ST_PointFromText('{longitude},{latitude}', ','), z.geometry) AS distance FROM {temp_table_name} as z ORDER BY distance ASC LIMIT {max_commune_number}\n",
    "     \"\"\")\n",
    "    return nearest_commune_df"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-11-25T10:24:10.024048Z",
     "start_time": "2024-11-25T10:24:09.997552Z"
    }
   },
   "id": "2a527d00100f901d",
   "outputs": [],
   "execution_count": 5
  },
  {
   "cell_type": "code",
   "source": [
    "# the gps coordinates for kremlin-Bicetre is 48.8100° N, 2.3539° E\n",
    "\n",
    "kb_latitude = \"48.8100\"\n",
    "kb_longitude = \"2.3539\""
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-11-25T10:24:16.708044Z",
     "start_time": "2024-11-25T10:24:16.690149Z"
    }
   },
   "id": "473adcb6d72b74d4",
   "outputs": [],
   "execution_count": 6
  },
  {
   "cell_type": "code",
   "source": [
    "\n",
    "kb_nearest_shape_df = get_nearest_commune(fr_commune_df,kb_latitude,kb_longitude,10)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-11-25T10:24:19.792999Z",
     "start_time": "2024-11-25T10:24:19.573520Z"
    }
   },
   "id": "5003cd8c6aa042e9",
   "outputs": [],
   "execution_count": 7
  },
  {
   "cell_type": "code",
   "source": [
    "%%time\n",
    "\n",
    "kb_nearest_shape_df.show()\n",
    "kb_nearest_shape_df.count()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-11-25T10:24:24.657560Z",
     "start_time": "2024-11-25T10:24:21.515387Z"
    }
   },
   "id": "d1ea6fde05bbf929",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+-----+------------------+\n",
      "|      commune_name|insee|          distance|\n",
      "+------------------+-----+------------------+\n",
      "|Le Kremlin-Bicêtre|94043|198.60307108585405|\n",
      "|          Gentilly|94037| 798.3521490770968|\n",
      "|           Arcueil|94003|1543.0937442695515|\n",
      "|         Villejuif|94076| 2007.793912679607|\n",
      "|    Ivry-sur-Seine|94041| 2489.634383841373|\n",
      "|            Cachan|94016| 2590.828517555236|\n",
      "|         Montrouge|92049| 2750.714176859015|\n",
      "|           Bagneux|92007| 3462.091511432535|\n",
      "|   Vitry-sur-Seine|94081|3845.1624363327196|\n",
      "|   L'Haÿ-les-Roses|94038| 3942.190017739479|\n",
      "+------------------+-----+------------------+\n",
      "\n",
      "CPU times: total: 0 ns\n",
      "Wall time: 3.13 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 8
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Read write GeoParquet\n",
    "GeoParquet is an **incubating Open Geospatial Consortium (OGC) standard** that adds interoperable geospatial types `(Point, Line, Polygon)` to Parquet. Currently(16/04/2024), the stable version is 1.0.0\n",
    "You can find the official site of geo-parquet [here](https://geoparquet.org/)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "284575701e4b1f43"
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "clean_fr_commune_df = fr_commune_df.withColumn(\"clean_nom\",trim(col(\"nom\"))).withColumn(\"clean_insee\",trim(col(\"insee\"))).drop(\"nom\").drop(\"insee\").withColumnRenamed(\"clean_nom\",\"nom\").withColumnRenamed(\"clean_insee\",\"insee\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-16T09:44:45.700151638Z",
     "start_time": "2024-04-16T09:44:45.528249357Z"
    }
   },
   "id": "5062dc628135f3e0"
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+--------------------+-----------------+-----+\n",
      "|            geometry|           wikipedia|             surf_ha|              nom|insee|\n",
      "+--------------------+--------------------+--------------------+-----------------+-----+\n",
      "|POLYGON ((9.32016...|fr:Pie-d'Orezza  ...|     573.00000000...|     Pie-d'Orezza|2B222|\n",
      "|POLYGON ((9.20010...|fr:Lano          ...|     824.00000000...|             Lano|2B137|\n",
      "|POLYGON ((9.27757...|fr:Cambia        ...|     833.00000000...|           Cambia|2B051|\n",
      "|POLYGON ((9.25119...|fr:Érone         ...|     393.00000000...|            Érone|2B106|\n",
      "|POLYGON ((9.28339...|fr:Oletta        ...|    2674.00000000...|           Oletta|2B185|\n",
      "|POLYGON ((9.30951...|fr:Canari (Haute-...|    1678.00000000...|           Canari|2B058|\n",
      "|POLYGON ((9.30101...|fr:Olmeta-di-Tuda...|    1753.00000000...|   Olmeta-di-Tuda|2B188|\n",
      "|POLYGON ((9.32662...|fr:Campana       ...|     236.00000000...|          Campana|2B052|\n",
      "|POLYGON ((9.33944...|fr:Carcheto-Brust...|     525.00000000...|Carcheto-Brustico|2B063|\n",
      "|POLYGON ((9.34478...|fr:Ampriani      ...|     230.00000000...|         Ampriani|2B015|\n",
      "|POLYGON ((9.33451...|fr:Pianello      ...|    1677.00000000...|         Pianello|2B213|\n",
      "|POLYGON ((9.32604...|fr:Zuani         ...|     518.00000000...|            Zuani|2B364|\n",
      "|POLYGON ((9.33795...|fr:Pietraserena  ...|     678.00000000...|     Pietraserena|2B226|\n",
      "|POLYGON ((9.33181...|fr:Piedipartino  ...|     326.00000000...|     Piedipartino|2B221|\n",
      "|POLYGON ((2.58733...|fr:Montbolo      ...|    2230.00000000...|         Montbolo|66113|\n",
      "|POLYGON ((1.96641...|fr:Targasonne    ...|     787.00000000...|       Targasonne|66202|\n",
      "|POLYGON ((2.86121...|fr:L'Albère      ...|    1620.00000000...|         L'Albère|66001|\n",
      "|POLYGON ((2.11418...|fr:Mont-Louis (Py...|      38.00000000...|       Mont-Louis|66117|\n",
      "|POLYGON ((1.98630...|fr:Estavar       ...|     926.00000000...|          Estavar|66072|\n",
      "|POLYGON ((2.00509...|fr:Égat          ...|     451.00000000...|             Égat|66064|\n",
      "+--------------------+--------------------+--------------------+-----------------+-----+\n"
     ]
    }
   ],
   "source": [
    "clean_fr_commune_df.show()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-16T09:45:00.553487428Z",
     "start_time": "2024-04-16T09:44:58.007347319Z"
    }
   },
   "id": "eba878d7a6f2d1c0"
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "fr_commune_geoparquet_file_path = \"/home/pengfei/data_set/kaggle/geospatial/communes_fr_geoparquet\"\n",
    "clean_fr_commune_df.write.format(\"geoparquet\").option(\"geoparquet.version\",\"1.0.0\").save(fr_commune_geoparquet_file_path)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-16T09:48:00.342320453Z",
     "start_time": "2024-04-16T09:47:49.119326543Z"
    }
   },
   "id": "642ed0cfc2e10ad1"
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\t/home/pengfei/data_set/kaggle/geospatial/communes_fr_geoparquet/_SUCCESS\r\n",
      "2.3M\t/home/pengfei/data_set/kaggle/geospatial/communes_fr_geoparquet/.part-00000-82765f1e-fe4e-4e74-81e2-01fd73bcdb34-c000.snappy.parquet.crc\r\n",
      "4.0K\t/home/pengfei/data_set/kaggle/geospatial/communes_fr_geoparquet/._SUCCESS.crc\r\n",
      "291M\t/home/pengfei/data_set/kaggle/geospatial/communes_fr_geoparquet/part-00000-82765f1e-fe4e-4e74-81e2-01fd73bcdb34-c000.snappy.parquet\r\n",
      "294M\t/home/pengfei/data_set/kaggle/geospatial/communes_fr_geoparquet\r\n"
     ]
    }
   ],
   "source": [
    "! du -ah /home/pengfei/data_set/kaggle/geospatial/communes_fr_geoparquet"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-16T10:24:08.581952184Z",
     "start_time": "2024-04-16T10:24:07.501643375Z"
    }
   },
   "id": "3d937cc14fff10b4"
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [
    "geo_parquet_df = sedona.read.format(\"geoparquet\").load(fr_commune_geoparquet_file_path)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-16T11:15:25.126938309Z",
     "start_time": "2024-04-16T11:15:24.545488491Z"
    }
   },
   "id": "15b7d4ec1ea6b867"
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+--------------------+-----------------+-----+\n",
      "|            geometry|           wikipedia|             surf_ha|              nom|insee|\n",
      "+--------------------+--------------------+--------------------+-----------------+-----+\n",
      "|POLYGON ((9.32016...|fr:Pie-d'Orezza  ...|     573.00000000...|     Pie-d'Orezza|2B222|\n",
      "|POLYGON ((9.20010...|fr:Lano          ...|     824.00000000...|             Lano|2B137|\n",
      "|POLYGON ((9.27757...|fr:Cambia        ...|     833.00000000...|           Cambia|2B051|\n",
      "|POLYGON ((9.25119...|fr:Érone         ...|     393.00000000...|            Érone|2B106|\n",
      "|POLYGON ((9.28339...|fr:Oletta        ...|    2674.00000000...|           Oletta|2B185|\n",
      "|POLYGON ((9.30951...|fr:Canari (Haute-...|    1678.00000000...|           Canari|2B058|\n",
      "|POLYGON ((9.30101...|fr:Olmeta-di-Tuda...|    1753.00000000...|   Olmeta-di-Tuda|2B188|\n",
      "|POLYGON ((9.32662...|fr:Campana       ...|     236.00000000...|          Campana|2B052|\n",
      "|POLYGON ((9.33944...|fr:Carcheto-Brust...|     525.00000000...|Carcheto-Brustico|2B063|\n",
      "|POLYGON ((9.34478...|fr:Ampriani      ...|     230.00000000...|         Ampriani|2B015|\n",
      "|POLYGON ((9.33451...|fr:Pianello      ...|    1677.00000000...|         Pianello|2B213|\n",
      "|POLYGON ((9.32604...|fr:Zuani         ...|     518.00000000...|            Zuani|2B364|\n",
      "|POLYGON ((9.33795...|fr:Pietraserena  ...|     678.00000000...|     Pietraserena|2B226|\n",
      "|POLYGON ((9.33181...|fr:Piedipartino  ...|     326.00000000...|     Piedipartino|2B221|\n",
      "|POLYGON ((2.58733...|fr:Montbolo      ...|    2230.00000000...|         Montbolo|66113|\n",
      "|POLYGON ((1.96641...|fr:Targasonne    ...|     787.00000000...|       Targasonne|66202|\n",
      "|POLYGON ((2.86121...|fr:L'Albère      ...|    1620.00000000...|         L'Albère|66001|\n",
      "|POLYGON ((2.11418...|fr:Mont-Louis (Py...|      38.00000000...|       Mont-Louis|66117|\n",
      "|POLYGON ((1.98630...|fr:Estavar       ...|     926.00000000...|          Estavar|66072|\n",
      "|POLYGON ((2.00509...|fr:Égat          ...|     451.00000000...|             Égat|66064|\n",
      "+--------------------+--------------------+--------------------+-----------------+-----+\n",
      "24/04/16 13:15:50 WARN GeoParquetFileFormat: GeoParquet currently does not support vectorized reader. Falling back to parquet-mr\n",
      "24/04/16 13:15:50 WARN GeoParquetFileFormat: GeoParquet currently does not support vectorized reader. Falling back to parquet-mr\n",
      "24/04/16 13:15:50 WARN GeoParquetFileFormat: GeoParquet currently does not support vectorized reader. Falling back to parquet-mr\n",
      "24/04/16 13:15:50 WARN GeoParquetFileFormat: GeoParquet currently does not support vectorized reader. Falling back to parquet-mr\n"
     ]
    },
    {
     "data": {
      "text/plain": "34955"
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "geo_parquet_df.show()\n",
    "geo_parquet_df.count()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-16T11:15:51.114646330Z",
     "start_time": "2024-04-16T11:15:47.005268357Z"
    }
   },
   "id": "d49a4ba1ca75ee15"
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [],
   "source": [
    "kb_nearest_parquet_df = get_nearest_commune(geo_parquet_df,kb_latitude,kb_longitude,10)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-16T11:26:07.496145072Z",
     "start_time": "2024-04-16T11:26:07.426289738Z"
    }
   },
   "id": "2967242bc44c12a0"
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+-----+------------------+\n",
      "|      commune_name|insee|          distance|\n",
      "+------------------+-----+------------------+\n",
      "|Le Kremlin-Bicêtre|94043|255.77950075329835|\n",
      "|          Gentilly|94037| 1138.204118880015|\n",
      "|         Villejuif|94076|2067.5242470555963|\n",
      "|           Arcueil|94003| 2269.505672821453|\n",
      "|            Cachan|94016|3169.7694895288837|\n",
      "|    Ivry-sur-Seine|94041| 3769.348960915047|\n",
      "|         Montrouge|92049| 4124.301376321017|\n",
      "|   L'Haÿ-les-Roses|94038| 4166.688028197553|\n",
      "|    Chevilly-Larue|94021| 4789.020724647998|\n",
      "|           Bagneux|92007|  5041.99634269013|\n",
      "+------------------+-----+------------------+\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 18:=============================>                            (2 + 2) / 4]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 9.98 ms, sys: 6.56 ms, total: 16.5 ms\n",
      "Wall time: 6.6 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": "10"
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "kb_nearest_parquet_df.show()\n",
    "kb_nearest_parquet_df.count()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-16T11:26:18.436106849Z",
     "start_time": "2024-04-16T11:26:11.802682371Z"
    }
   },
   "id": "e8f5da85bd8a205e"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Custom metadata in geo parquet"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "34ff512153c4ee98"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Compare the result of shape file and geo parquet, we don't gain too many things\n",
    "\n",
    "| file format | disk space | distance (in sec) |\n",
    "|-------------|------------|-------------------|\n",
    "| shape file  | 301        | 7,45              |\n",
    "| geoparquet  | 294        | 6,60              |"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "4902b5fef6331016"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Read write GeoJSON(Geographic JavaScript Object Notation)\n",
    "\n",
    "Sedona can read geojson easily, but can't write geojson. Geo pandas can write geojson. But it can't support large \n",
    "data frame. Below are two examples. In the first, we create a simple geo dataframe. It works without problem.\n",
    "The second does work at all. We have an oom error."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "717a0ad83de39384"
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   id name                 geometry\n",
      "0   1    A  POINT (1.00000 1.00000)\n",
      "1   2    B  POINT (2.00000 2.00000)\n",
      "2   3    C  POINT (3.00000 3.00000)\n"
     ]
    }
   ],
   "source": [
    "from shapely import Point\n",
    "\n",
    "data = {\n",
    "    'id': [1, 2, 3],\n",
    "    'name': ['A', 'B', 'C'],\n",
    "    'geometry': [Point(1, 1), Point(2, 2), Point(3, 3)]\n",
    "}\n",
    "\n",
    "fr_commune_geoj_file_path = \"/home/pengfei/data_set/kaggle/geospatial/communes_fr_geojson.json\"\n",
    "\n",
    "gdf = gpd.GeoDataFrame(data, crs=\"EPSG:4326\")\n",
    "\n",
    "print(gdf.head())\n",
    "\n",
    "# Write GeoDataFrame to GeoJSON file\n",
    "gdf.to_file(fr_commune_geoj_file_path, driver='GeoJSON')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-15T14:29:14.499343553Z",
     "start_time": "2024-04-15T14:29:14.432043733Z"
    }
   },
   "id": "2b4c79db7ae1c04c"
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "|            geometry|               insee|                 nom|           wikipedia|             surf_ha|\n",
      "+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "|POLYGON ((9.32016...|2B222            ...|Pie-d'Orezza     ...|fr:Pie-d'Orezza  ...|     573.00000000...|\n",
      "|POLYGON ((9.20010...|2B137            ...|Lano             ...|fr:Lano          ...|     824.00000000...|\n",
      "|POLYGON ((9.27757...|2B051            ...|Cambia           ...|fr:Cambia        ...|     833.00000000...|\n",
      "|POLYGON ((9.25119...|2B106            ...|Érone            ...|fr:Érone         ...|     393.00000000...|\n",
      "|POLYGON ((9.28339...|2B185            ...|Oletta           ...|fr:Oletta        ...|    2674.00000000...|\n",
      "|POLYGON ((9.30951...|2B058            ...|Canari           ...|fr:Canari (Haute-...|    1678.00000000...|\n",
      "|POLYGON ((9.30101...|2B188            ...|Olmeta-di-Tuda   ...|fr:Olmeta-di-Tuda...|    1753.00000000...|\n",
      "|POLYGON ((9.32662...|2B052            ...|Campana          ...|fr:Campana       ...|     236.00000000...|\n",
      "|POLYGON ((9.33944...|2B063            ...|Carcheto-Brustico...|fr:Carcheto-Brust...|     525.00000000...|\n",
      "|POLYGON ((9.34478...|2B015            ...|Ampriani         ...|fr:Ampriani      ...|     230.00000000...|\n",
      "|POLYGON ((9.33451...|2B213            ...|Pianello         ...|fr:Pianello      ...|    1677.00000000...|\n",
      "|POLYGON ((9.32604...|2B364            ...|Zuani            ...|fr:Zuani         ...|     518.00000000...|\n",
      "|POLYGON ((9.33795...|2B226            ...|Pietraserena     ...|fr:Pietraserena  ...|     678.00000000...|\n",
      "|POLYGON ((9.33181...|2B221            ...|Piedipartino     ...|fr:Piedipartino  ...|     326.00000000...|\n",
      "|POLYGON ((2.58733...|66113            ...|Montbolo         ...|fr:Montbolo      ...|    2230.00000000...|\n",
      "|POLYGON ((1.96641...|66202            ...|Targasonne       ...|fr:Targasonne    ...|     787.00000000...|\n",
      "|POLYGON ((2.86121...|66001            ...|L'Albère         ...|fr:L'Albère      ...|    1620.00000000...|\n",
      "|POLYGON ((2.11418...|66117            ...|Mont-Louis       ...|fr:Mont-Louis (Py...|      38.00000000...|\n",
      "|POLYGON ((1.98630...|66072            ...|Estavar          ...|fr:Estavar       ...|     926.00000000...|\n",
      "|POLYGON ((2.00509...|66064            ...|Égat             ...|fr:Égat          ...|     451.00000000...|\n",
      "+--------------------+--------------------+--------------------+--------------------+--------------------+\n"
     ]
    }
   ],
   "source": [
    "fr_commune_df.show()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-15T14:28:42.200495626Z",
     "start_time": "2024-04-15T14:28:41.861630313Z"
    }
   },
   "id": "14f2e6c68909fd64"
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "outputs": [],
   "source": [
    "from shapely import Polygon\n",
    "from pyspark.sql.functions import collect_list\n",
    "\n",
    "\n",
    "def get_geopandas_df(spark_df:DataFrame):\n",
    "    # Convert Spark DataFrame to Pandas DataFrame\n",
    "    pandas_df = spark_df.toPandas()\n",
    "\n",
    "    # Create a GeoPandas DataFrame from the Pandas DataFrame\n",
    "    # Make sure to create Shapely geometry objects from the geometry column\n",
    "    pandas_df['geometry'] = pandas_df['geometry'].apply(lambda x: Polygon(eval(x)))\n",
    "    geo_df = gpd.GeoDataFrame(pandas_df, geometry='geometry')\n",
    "    \n",
    "    return geo_df"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-15T14:03:02.670598870Z",
     "start_time": "2024-04-15T14:03:02.628166906Z"
    }
   },
   "id": "d2110268a0a6ce4e"
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 21:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24/04/15 16:03:16 ERROR Executor: Exception in task 0.0 in stage 21.0 (TID 19)\n",
      "org.apache.spark.SparkException: Kryo serialization failed: Buffer overflow. Available: 0, required: 67108864. To avoid this, increase spark.kryoserializer.buffer.max value.\n",
      "\tat org.apache.spark.serializer.KryoSerializerInstance.serialize(KryoSerializer.scala:391)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:593)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "Caused by: com.esotericsoftware.kryo.KryoException: Buffer overflow. Available: 0, required: 67108864\n",
      "\tat com.esotericsoftware.kryo.io.Output.require(Output.java:167)\n",
      "\tat com.esotericsoftware.kryo.io.Output.writeBytes(Output.java:251)\n",
      "\tat com.esotericsoftware.kryo.io.Output.writeBytes(Output.java:237)\n",
      "\tat com.esotericsoftware.kryo.serializers.DefaultArraySerializers$ByteArraySerializer.write(DefaultArraySerializers.java:49)\n",
      "\tat com.esotericsoftware.kryo.serializers.DefaultArraySerializers$ByteArraySerializer.write(DefaultArraySerializers.java:38)\n",
      "\tat com.esotericsoftware.kryo.Kryo.writeClassAndObject(Kryo.java:651)\n",
      "\tat com.twitter.chill.Tuple2Serializer.write(TupleSerializers.scala:37)\n",
      "\tat com.twitter.chill.Tuple2Serializer.write(TupleSerializers.scala:33)\n",
      "\tat com.esotericsoftware.kryo.Kryo.writeClassAndObject(Kryo.java:651)\n",
      "\tat com.esotericsoftware.kryo.serializers.DefaultArraySerializers$ObjectArraySerializer.write(DefaultArraySerializers.java:361)\n",
      "\tat com.esotericsoftware.kryo.serializers.DefaultArraySerializers$ObjectArraySerializer.write(DefaultArraySerializers.java:302)\n",
      "\tat com.esotericsoftware.kryo.Kryo.writeClassAndObject(Kryo.java:651)\n",
      "\tat org.apache.spark.serializer.KryoSerializerInstance.serialize(KryoSerializer.scala:387)\n",
      "\t... 4 more\n",
      "24/04/15 16:03:16 WARN TaskSetManager: Lost task 0.0 in stage 21.0 (TID 19) (10.50.2.80 executor driver): org.apache.spark.SparkException: Kryo serialization failed: Buffer overflow. Available: 0, required: 67108864. To avoid this, increase spark.kryoserializer.buffer.max value.\n",
      "\tat org.apache.spark.serializer.KryoSerializerInstance.serialize(KryoSerializer.scala:391)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:593)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "Caused by: com.esotericsoftware.kryo.KryoException: Buffer overflow. Available: 0, required: 67108864\n",
      "\tat com.esotericsoftware.kryo.io.Output.require(Output.java:167)\n",
      "\tat com.esotericsoftware.kryo.io.Output.writeBytes(Output.java:251)\n",
      "\tat com.esotericsoftware.kryo.io.Output.writeBytes(Output.java:237)\n",
      "\tat com.esotericsoftware.kryo.serializers.DefaultArraySerializers$ByteArraySerializer.write(DefaultArraySerializers.java:49)\n",
      "\tat com.esotericsoftware.kryo.serializers.DefaultArraySerializers$ByteArraySerializer.write(DefaultArraySerializers.java:38)\n",
      "\tat com.esotericsoftware.kryo.Kryo.writeClassAndObject(Kryo.java:651)\n",
      "\tat com.twitter.chill.Tuple2Serializer.write(TupleSerializers.scala:37)\n",
      "\tat com.twitter.chill.Tuple2Serializer.write(TupleSerializers.scala:33)\n",
      "\tat com.esotericsoftware.kryo.Kryo.writeClassAndObject(Kryo.java:651)\n",
      "\tat com.esotericsoftware.kryo.serializers.DefaultArraySerializers$ObjectArraySerializer.write(DefaultArraySerializers.java:361)\n",
      "\tat com.esotericsoftware.kryo.serializers.DefaultArraySerializers$ObjectArraySerializer.write(DefaultArraySerializers.java:302)\n",
      "\tat com.esotericsoftware.kryo.Kryo.writeClassAndObject(Kryo.java:651)\n",
      "\tat org.apache.spark.serializer.KryoSerializerInstance.serialize(KryoSerializer.scala:387)\n",
      "\t... 4 more\n",
      "\n",
      "24/04/15 16:03:16 ERROR TaskSetManager: Task 0 in stage 21.0 failed 1 times; aborting job\n"
     ]
    },
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o49.collectToPython.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 21.0 failed 1 times, most recent failure: Lost task 0.0 in stage 21.0 (TID 19) (10.50.2.80 executor driver): org.apache.spark.SparkException: Kryo serialization failed: Buffer overflow. Available: 0, required: 67108864. To avoid this, increase spark.kryoserializer.buffer.max value.\n\tat org.apache.spark.serializer.KryoSerializerInstance.serialize(KryoSerializer.scala:391)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:593)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n\tat java.base/java.lang.Thread.run(Thread.java:829)\nCaused by: com.esotericsoftware.kryo.KryoException: Buffer overflow. Available: 0, required: 67108864\n\tat com.esotericsoftware.kryo.io.Output.require(Output.java:167)\n\tat com.esotericsoftware.kryo.io.Output.writeBytes(Output.java:251)\n\tat com.esotericsoftware.kryo.io.Output.writeBytes(Output.java:237)\n\tat com.esotericsoftware.kryo.serializers.DefaultArraySerializers$ByteArraySerializer.write(DefaultArraySerializers.java:49)\n\tat com.esotericsoftware.kryo.serializers.DefaultArraySerializers$ByteArraySerializer.write(DefaultArraySerializers.java:38)\n\tat com.esotericsoftware.kryo.Kryo.writeClassAndObject(Kryo.java:651)\n\tat com.twitter.chill.Tuple2Serializer.write(TupleSerializers.scala:37)\n\tat com.twitter.chill.Tuple2Serializer.write(TupleSerializers.scala:33)\n\tat com.esotericsoftware.kryo.Kryo.writeClassAndObject(Kryo.java:651)\n\tat com.esotericsoftware.kryo.serializers.DefaultArraySerializers$ObjectArraySerializer.write(DefaultArraySerializers.java:361)\n\tat com.esotericsoftware.kryo.serializers.DefaultArraySerializers$ObjectArraySerializer.write(DefaultArraySerializers.java:302)\n\tat com.esotericsoftware.kryo.Kryo.writeClassAndObject(Kryo.java:651)\n\tat org.apache.spark.serializer.KryoSerializerInstance.serialize(KryoSerializer.scala:387)\n\t... 4 more\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2672)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2608)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2607)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2607)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1182)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1182)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1182)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2860)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2802)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2791)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:952)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2228)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2249)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2268)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2293)\n\tat org.apache.spark.rdd.RDD.$anonfun$collect$1(RDD.scala:1021)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:406)\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:1020)\n\tat org.apache.spark.sql.execution.SparkPlan.executeCollect(SparkPlan.scala:424)\n\tat org.apache.spark.sql.Dataset.$anonfun$collectToPython$1(Dataset.scala:3688)\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$2(Dataset.scala:3858)\n\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:510)\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:3856)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:109)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:169)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:95)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:779)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:3856)\n\tat org.apache.spark.sql.Dataset.collectToPython(Dataset.scala:3685)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:829)\nCaused by: org.apache.spark.SparkException: Kryo serialization failed: Buffer overflow. Available: 0, required: 67108864. To avoid this, increase spark.kryoserializer.buffer.max value.\n\tat org.apache.spark.serializer.KryoSerializerInstance.serialize(KryoSerializer.scala:391)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:593)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n\t... 1 more\nCaused by: com.esotericsoftware.kryo.KryoException: Buffer overflow. Available: 0, required: 67108864\n\tat com.esotericsoftware.kryo.io.Output.require(Output.java:167)\n\tat com.esotericsoftware.kryo.io.Output.writeBytes(Output.java:251)\n\tat com.esotericsoftware.kryo.io.Output.writeBytes(Output.java:237)\n\tat com.esotericsoftware.kryo.serializers.DefaultArraySerializers$ByteArraySerializer.write(DefaultArraySerializers.java:49)\n\tat com.esotericsoftware.kryo.serializers.DefaultArraySerializers$ByteArraySerializer.write(DefaultArraySerializers.java:38)\n\tat com.esotericsoftware.kryo.Kryo.writeClassAndObject(Kryo.java:651)\n\tat com.twitter.chill.Tuple2Serializer.write(TupleSerializers.scala:37)\n\tat com.twitter.chill.Tuple2Serializer.write(TupleSerializers.scala:33)\n\tat com.esotericsoftware.kryo.Kryo.writeClassAndObject(Kryo.java:651)\n\tat com.esotericsoftware.kryo.serializers.DefaultArraySerializers$ObjectArraySerializer.write(DefaultArraySerializers.java:361)\n\tat com.esotericsoftware.kryo.serializers.DefaultArraySerializers$ObjectArraySerializer.write(DefaultArraySerializers.java:302)\n\tat com.esotericsoftware.kryo.Kryo.writeClassAndObject(Kryo.java:651)\n\tat org.apache.spark.serializer.KryoSerializerInstance.serialize(KryoSerializer.scala:387)\n\t... 4 more\n",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mPy4JJavaError\u001B[0m                             Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[24], line 1\u001B[0m\n\u001B[0;32m----> 1\u001B[0m gdf \u001B[38;5;241m=\u001B[39m \u001B[43mget_geopandas_df\u001B[49m\u001B[43m(\u001B[49m\u001B[43mfr_commune_df\u001B[49m\u001B[43m)\u001B[49m\n",
      "Cell \u001B[0;32mIn[23], line 7\u001B[0m, in \u001B[0;36mget_geopandas_df\u001B[0;34m(spark_df)\u001B[0m\n\u001B[1;32m      5\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mget_geopandas_df\u001B[39m(spark_df:DataFrame):\n\u001B[1;32m      6\u001B[0m     \u001B[38;5;66;03m# Convert Spark DataFrame to Pandas DataFrame\u001B[39;00m\n\u001B[0;32m----> 7\u001B[0m     pandas_df \u001B[38;5;241m=\u001B[39m \u001B[43mspark_df\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtoPandas\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m      9\u001B[0m     \u001B[38;5;66;03m# Create a GeoPandas DataFrame from the Pandas DataFrame\u001B[39;00m\n\u001B[1;32m     10\u001B[0m     \u001B[38;5;66;03m# Make sure to create Shapely geometry objects from the geometry column\u001B[39;00m\n\u001B[1;32m     11\u001B[0m     pandas_df[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mgeometry\u001B[39m\u001B[38;5;124m'\u001B[39m] \u001B[38;5;241m=\u001B[39m pandas_df[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mgeometry\u001B[39m\u001B[38;5;124m'\u001B[39m]\u001B[38;5;241m.\u001B[39mapply(\u001B[38;5;28;01mlambda\u001B[39;00m x: Polygon(\u001B[38;5;28meval\u001B[39m(x)))\n",
      "File \u001B[0;32m~/.cache/pypoetry/virtualenvs/geoparquetandsedona-uqJLJxB7-py3.10/lib/python3.10/site-packages/pyspark/sql/pandas/conversion.py:205\u001B[0m, in \u001B[0;36mPandasConversionMixin.toPandas\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    202\u001B[0m             \u001B[38;5;28;01mraise\u001B[39;00m\n\u001B[1;32m    204\u001B[0m \u001B[38;5;66;03m# Below is toPandas without Arrow optimization.\u001B[39;00m\n\u001B[0;32m--> 205\u001B[0m pdf \u001B[38;5;241m=\u001B[39m pd\u001B[38;5;241m.\u001B[39mDataFrame\u001B[38;5;241m.\u001B[39mfrom_records(\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcollect\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m, columns\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcolumns)\n\u001B[1;32m    206\u001B[0m column_counter \u001B[38;5;241m=\u001B[39m Counter(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcolumns)\n\u001B[1;32m    208\u001B[0m corrected_dtypes: List[Optional[Type]] \u001B[38;5;241m=\u001B[39m [\u001B[38;5;28;01mNone\u001B[39;00m] \u001B[38;5;241m*\u001B[39m \u001B[38;5;28mlen\u001B[39m(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mschema)\n",
      "File \u001B[0;32m~/.cache/pypoetry/virtualenvs/geoparquetandsedona-uqJLJxB7-py3.10/lib/python3.10/site-packages/pyspark/sql/dataframe.py:817\u001B[0m, in \u001B[0;36mDataFrame.collect\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    807\u001B[0m \u001B[38;5;250m\u001B[39m\u001B[38;5;124;03m\"\"\"Returns all the records as a list of :class:`Row`.\u001B[39;00m\n\u001B[1;32m    808\u001B[0m \n\u001B[1;32m    809\u001B[0m \u001B[38;5;124;03m.. versionadded:: 1.3.0\u001B[39;00m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    814\u001B[0m \u001B[38;5;124;03m[Row(age=2, name='Alice'), Row(age=5, name='Bob')]\u001B[39;00m\n\u001B[1;32m    815\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m    816\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m SCCallSiteSync(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_sc):\n\u001B[0;32m--> 817\u001B[0m     sock_info \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_jdf\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcollectToPython\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    818\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mlist\u001B[39m(_load_from_socket(sock_info, BatchedSerializer(CPickleSerializer())))\n",
      "File \u001B[0;32m~/.cache/pypoetry/virtualenvs/geoparquetandsedona-uqJLJxB7-py3.10/lib/python3.10/site-packages/py4j/java_gateway.py:1321\u001B[0m, in \u001B[0;36mJavaMember.__call__\u001B[0;34m(self, *args)\u001B[0m\n\u001B[1;32m   1315\u001B[0m command \u001B[38;5;241m=\u001B[39m proto\u001B[38;5;241m.\u001B[39mCALL_COMMAND_NAME \u001B[38;5;241m+\u001B[39m\\\n\u001B[1;32m   1316\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcommand_header \u001B[38;5;241m+\u001B[39m\\\n\u001B[1;32m   1317\u001B[0m     args_command \u001B[38;5;241m+\u001B[39m\\\n\u001B[1;32m   1318\u001B[0m     proto\u001B[38;5;241m.\u001B[39mEND_COMMAND_PART\n\u001B[1;32m   1320\u001B[0m answer \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mgateway_client\u001B[38;5;241m.\u001B[39msend_command(command)\n\u001B[0;32m-> 1321\u001B[0m return_value \u001B[38;5;241m=\u001B[39m \u001B[43mget_return_value\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m   1322\u001B[0m \u001B[43m    \u001B[49m\u001B[43manswer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mgateway_client\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtarget_id\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mname\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1324\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m temp_arg \u001B[38;5;129;01min\u001B[39;00m temp_args:\n\u001B[1;32m   1325\u001B[0m     temp_arg\u001B[38;5;241m.\u001B[39m_detach()\n",
      "File \u001B[0;32m~/.cache/pypoetry/virtualenvs/geoparquetandsedona-uqJLJxB7-py3.10/lib/python3.10/site-packages/pyspark/sql/utils.py:190\u001B[0m, in \u001B[0;36mcapture_sql_exception.<locals>.deco\u001B[0;34m(*a, **kw)\u001B[0m\n\u001B[1;32m    188\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mdeco\u001B[39m(\u001B[38;5;241m*\u001B[39ma: Any, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkw: Any) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Any:\n\u001B[1;32m    189\u001B[0m     \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m--> 190\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mf\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43ma\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkw\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    191\u001B[0m     \u001B[38;5;28;01mexcept\u001B[39;00m Py4JJavaError \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[1;32m    192\u001B[0m         converted \u001B[38;5;241m=\u001B[39m convert_exception(e\u001B[38;5;241m.\u001B[39mjava_exception)\n",
      "File \u001B[0;32m~/.cache/pypoetry/virtualenvs/geoparquetandsedona-uqJLJxB7-py3.10/lib/python3.10/site-packages/py4j/protocol.py:326\u001B[0m, in \u001B[0;36mget_return_value\u001B[0;34m(answer, gateway_client, target_id, name)\u001B[0m\n\u001B[1;32m    324\u001B[0m value \u001B[38;5;241m=\u001B[39m OUTPUT_CONVERTER[\u001B[38;5;28mtype\u001B[39m](answer[\u001B[38;5;241m2\u001B[39m:], gateway_client)\n\u001B[1;32m    325\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m answer[\u001B[38;5;241m1\u001B[39m] \u001B[38;5;241m==\u001B[39m REFERENCE_TYPE:\n\u001B[0;32m--> 326\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m Py4JJavaError(\n\u001B[1;32m    327\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mAn error occurred while calling \u001B[39m\u001B[38;5;132;01m{0}\u001B[39;00m\u001B[38;5;132;01m{1}\u001B[39;00m\u001B[38;5;132;01m{2}\u001B[39;00m\u001B[38;5;124m.\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;241m.\u001B[39m\n\u001B[1;32m    328\u001B[0m         \u001B[38;5;28mformat\u001B[39m(target_id, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m.\u001B[39m\u001B[38;5;124m\"\u001B[39m, name), value)\n\u001B[1;32m    329\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    330\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m Py4JError(\n\u001B[1;32m    331\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mAn error occurred while calling \u001B[39m\u001B[38;5;132;01m{0}\u001B[39;00m\u001B[38;5;132;01m{1}\u001B[39;00m\u001B[38;5;132;01m{2}\u001B[39;00m\u001B[38;5;124m. Trace:\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;132;01m{3}\u001B[39;00m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;241m.\u001B[39m\n\u001B[1;32m    332\u001B[0m         \u001B[38;5;28mformat\u001B[39m(target_id, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m.\u001B[39m\u001B[38;5;124m\"\u001B[39m, name, value))\n",
      "\u001B[0;31mPy4JJavaError\u001B[0m: An error occurred while calling o49.collectToPython.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 21.0 failed 1 times, most recent failure: Lost task 0.0 in stage 21.0 (TID 19) (10.50.2.80 executor driver): org.apache.spark.SparkException: Kryo serialization failed: Buffer overflow. Available: 0, required: 67108864. To avoid this, increase spark.kryoserializer.buffer.max value.\n\tat org.apache.spark.serializer.KryoSerializerInstance.serialize(KryoSerializer.scala:391)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:593)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n\tat java.base/java.lang.Thread.run(Thread.java:829)\nCaused by: com.esotericsoftware.kryo.KryoException: Buffer overflow. Available: 0, required: 67108864\n\tat com.esotericsoftware.kryo.io.Output.require(Output.java:167)\n\tat com.esotericsoftware.kryo.io.Output.writeBytes(Output.java:251)\n\tat com.esotericsoftware.kryo.io.Output.writeBytes(Output.java:237)\n\tat com.esotericsoftware.kryo.serializers.DefaultArraySerializers$ByteArraySerializer.write(DefaultArraySerializers.java:49)\n\tat com.esotericsoftware.kryo.serializers.DefaultArraySerializers$ByteArraySerializer.write(DefaultArraySerializers.java:38)\n\tat com.esotericsoftware.kryo.Kryo.writeClassAndObject(Kryo.java:651)\n\tat com.twitter.chill.Tuple2Serializer.write(TupleSerializers.scala:37)\n\tat com.twitter.chill.Tuple2Serializer.write(TupleSerializers.scala:33)\n\tat com.esotericsoftware.kryo.Kryo.writeClassAndObject(Kryo.java:651)\n\tat com.esotericsoftware.kryo.serializers.DefaultArraySerializers$ObjectArraySerializer.write(DefaultArraySerializers.java:361)\n\tat com.esotericsoftware.kryo.serializers.DefaultArraySerializers$ObjectArraySerializer.write(DefaultArraySerializers.java:302)\n\tat com.esotericsoftware.kryo.Kryo.writeClassAndObject(Kryo.java:651)\n\tat org.apache.spark.serializer.KryoSerializerInstance.serialize(KryoSerializer.scala:387)\n\t... 4 more\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2672)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2608)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2607)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2607)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1182)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1182)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1182)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2860)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2802)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2791)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:952)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2228)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2249)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2268)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2293)\n\tat org.apache.spark.rdd.RDD.$anonfun$collect$1(RDD.scala:1021)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:406)\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:1020)\n\tat org.apache.spark.sql.execution.SparkPlan.executeCollect(SparkPlan.scala:424)\n\tat org.apache.spark.sql.Dataset.$anonfun$collectToPython$1(Dataset.scala:3688)\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$2(Dataset.scala:3858)\n\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:510)\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:3856)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:109)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:169)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:95)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:779)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:3856)\n\tat org.apache.spark.sql.Dataset.collectToPython(Dataset.scala:3685)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:829)\nCaused by: org.apache.spark.SparkException: Kryo serialization failed: Buffer overflow. Available: 0, required: 67108864. To avoid this, increase spark.kryoserializer.buffer.max value.\n\tat org.apache.spark.serializer.KryoSerializerInstance.serialize(KryoSerializer.scala:391)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:593)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n\t... 1 more\nCaused by: com.esotericsoftware.kryo.KryoException: Buffer overflow. Available: 0, required: 67108864\n\tat com.esotericsoftware.kryo.io.Output.require(Output.java:167)\n\tat com.esotericsoftware.kryo.io.Output.writeBytes(Output.java:251)\n\tat com.esotericsoftware.kryo.io.Output.writeBytes(Output.java:237)\n\tat com.esotericsoftware.kryo.serializers.DefaultArraySerializers$ByteArraySerializer.write(DefaultArraySerializers.java:49)\n\tat com.esotericsoftware.kryo.serializers.DefaultArraySerializers$ByteArraySerializer.write(DefaultArraySerializers.java:38)\n\tat com.esotericsoftware.kryo.Kryo.writeClassAndObject(Kryo.java:651)\n\tat com.twitter.chill.Tuple2Serializer.write(TupleSerializers.scala:37)\n\tat com.twitter.chill.Tuple2Serializer.write(TupleSerializers.scala:33)\n\tat com.esotericsoftware.kryo.Kryo.writeClassAndObject(Kryo.java:651)\n\tat com.esotericsoftware.kryo.serializers.DefaultArraySerializers$ObjectArraySerializer.write(DefaultArraySerializers.java:361)\n\tat com.esotericsoftware.kryo.serializers.DefaultArraySerializers$ObjectArraySerializer.write(DefaultArraySerializers.java:302)\n\tat com.esotericsoftware.kryo.Kryo.writeClassAndObject(Kryo.java:651)\n\tat org.apache.spark.serializer.KryoSerializerInstance.serialize(KryoSerializer.scala:387)\n\t... 4 more\n"
     ]
    }
   ],
   "source": [
    "gdf = get_geopandas_df(fr_commune_df)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-15T14:03:16.401282159Z",
     "start_time": "2024-04-15T14:03:10.821930661Z"
    }
   },
   "id": "f1abd4b03fb11ee5"
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 28:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+-------+\n",
      "|            geometry|          properties|   type|\n",
      "+--------------------+--------------------+-------+\n",
      "|{[121.4961111, 25...|{Yungho, yungho, ...|Feature|\n",
      "|{[-72.233333, -37...|{Mulchen, mulchen...|Feature|\n",
      "|{[-73.6405556, 40...|{Oceanside, ocean...|Feature|\n",
      "|{[-70.966667, -32...|{Llaillay, llaill...|Feature|\n",
      "|{[35.6, 3.1166667...|{Lodwar, lodwar, ...|Feature|\n",
      "|{[10.1666667, 5.9...|{Bamenda, bamenda...|Feature|\n",
      "|{[-45.533333, -20...|{Arcos, arcos, br...|Feature|\n",
      "|{[-43.716944, -22...|{Seropédica, sero...|Feature|\n",
      "|{[-97.1413889, 32...|{Mansfield, mansf...|Feature|\n",
      "|{[-67.5419444, 10...|{Palo Negro, palo...|Feature|\n",
      "|{[-42.683333, -5....|{Demerval Lobão, ...|Feature|\n",
      "|{[-48.666667, -28...|{Imbituba, imbitu...|Feature|\n",
      "|{[-49.333333, -5....|{Itupiranga, itup...|Feature|\n",
      "|{[121.75, 24.7666...|{Ilan, ilan, tw, ...|Feature|\n",
      "|{[49.1825, 11.284...|{Bosaso, bosaso, ...|Feature|\n",
      "|{[64.570048, 31.8...|{Geresk, geresk, ...|Feature|\n",
      "|{[7.573271, 47.55...|{Basel, basel, ch...|Feature|\n",
      "|{[76.099444, 13.0...|{Hassan, hassan, ...|Feature|\n",
      "|{[17.637616, 47.6...|{Gyor, gyor, hu, ...|Feature|\n",
      "|{[6.92917, 43.660...|{Grasse, grasse, ...|Feature|\n",
      "+--------------------+--------------------+-------+\n",
      "only showing top 20 rows\n",
      "\n",
      "root\n",
      " |-- geometry: struct (nullable = true)\n",
      " |    |-- coordinates: array (nullable = true)\n",
      " |    |    |-- element: double (containsNull = true)\n",
      " |    |-- type: string (nullable = true)\n",
      " |-- properties: struct (nullable = true)\n",
      " |    |-- accentcity: string (nullable = true)\n",
      " |    |-- city: string (nullable = true)\n",
      " |    |-- country: string (nullable = true)\n",
      " |    |-- geopoint: array (nullable = true)\n",
      " |    |    |-- element: double (containsNull = true)\n",
      " |    |-- latitude: double (nullable = true)\n",
      " |    |-- longitude: double (nullable = true)\n",
      " |    |-- population: long (nullable = true)\n",
      " |    |-- region: string (nullable = true)\n",
      " |-- type: string (nullable = true)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "city_geoj_file_path = \"/home/pengfei/data_set/kaggle/geospatial/world-cities.geojson\"\n",
    "\n",
    "# read geo json\n",
    "# the selectExpr action Explode the envelope to get one feature per row.\n",
    "#  Unpack the features' struct. \n",
    "df = sedona.read.format(\"json\").option(\"multiLine\", \"true\").load(city_geoj_file_path) \\\n",
    " .selectExpr(\"explode(features) as features\")  \\\n",
    " .select(\"features.*\")  \n",
    " # .withColumn(\"prop0\", f.expr(\"properties['prop0']\")).drop(\"properties\").drop(\"type\")\n",
    "\n",
    "df.show()\n",
    "df.printSchema()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-15T14:35:25.513712656Z",
     "start_time": "2024-04-15T14:35:21.051507270Z"
    }
   },
   "id": "4d3fb1626fdcb953"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "1c0246b76fba9544"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
